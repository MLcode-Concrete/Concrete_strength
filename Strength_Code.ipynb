{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "VaG5RxKJEq6j",
        "outputId": "6c9dcf53-ac92-4562-95a5-5b0998d68a8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Cement  Pumice  SilicaFume  Powder       W/P   SP     CA/FA  Type  Strength\n",
              "0   450.0     0.0         0.0   450.0  0.400000  0.0  1.500000     1      60.0\n",
              "1   450.0     0.0         0.0   600.0  0.300000  0.0  1.500000     1      55.0\n",
              "2   382.5    67.5         0.0   600.0  0.300000  0.0  1.481481     1      45.0\n",
              "3   450.0    67.5         0.0   600.0  0.300000  0.0  1.498316     1      69.0\n",
              "4   500.0     0.0         0.0   600.0  0.300000  1.2  0.663542     2      48.0\n",
              "5   450.0    50.0         0.0   600.0  0.270000  1.2  0.660417     2      49.5\n",
              "6   400.0   100.0         0.0   600.0  0.240000  1.2  0.657292     2      41.0\n",
              "7   350.0   150.0         0.0   600.0  0.210000  1.2  0.655208     2      39.0\n",
              "8   500.0     0.0         0.0   600.0  0.333333  1.2  0.632524     2      42.0\n",
              "9   450.0    50.0         0.0   600.0  0.300000  1.2  0.629356     2      43.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3001c232-7243-48e0-a747-d4fc84d2a2b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cement</th>\n",
              "      <th>Pumice</th>\n",
              "      <th>SilicaFume</th>\n",
              "      <th>Powder</th>\n",
              "      <th>W/P</th>\n",
              "      <th>SP</th>\n",
              "      <th>CA/FA</th>\n",
              "      <th>Type</th>\n",
              "      <th>Strength</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>450.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>450.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>450.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>55.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>382.5</td>\n",
              "      <td>67.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.481481</td>\n",
              "      <td>1</td>\n",
              "      <td>45.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>450.0</td>\n",
              "      <td>67.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.498316</td>\n",
              "      <td>1</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.663542</td>\n",
              "      <td>2</td>\n",
              "      <td>48.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>450.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.660417</td>\n",
              "      <td>2</td>\n",
              "      <td>49.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>400.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.240000</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.657292</td>\n",
              "      <td>2</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>350.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.655208</td>\n",
              "      <td>2</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>500.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.632524</td>\n",
              "      <td>2</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>450.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.629356</td>\n",
              "      <td>2</td>\n",
              "      <td>43.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3001c232-7243-48e0-a747-d4fc84d2a2b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3001c232-7243-48e0-a747-d4fc84d2a2b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3001c232-7243-48e0-a747-d4fc84d2a2b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5ef2cf8e-147d-4d13-b096-e075acd97942\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ef2cf8e-147d-4d13-b096-e075acd97942')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5ef2cf8e-147d-4d13-b096-e075acd97942 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 304,\n  \"fields\": [\n    {\n      \"column\": \"Cement\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 107.16076741098455,\n        \"min\": 0.0,\n        \"max\": 650.0,\n        \"num_unique_values\": 92,\n        \"samples\": [\n          320.0,\n          200.0,\n          415.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pumice\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.81995860580584,\n        \"min\": 0.0,\n        \"max\": 450.0,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          22.5,\n          120.0,\n          73.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SilicaFume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.488478225185144,\n        \"min\": 0.0,\n        \"max\": 90.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.0,\n          90.0,\n          45.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Powder\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.85809303794584,\n        \"min\": 350.0,\n        \"max\": 650.0,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          580.0,\n          480.0,\n          452.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"W/P\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08250217049475703,\n        \"min\": 0.2,\n        \"max\": 0.642857143,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          0.21,\n          0.508888889,\n          0.399948294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1559767685705926,\n        \"min\": 0.0,\n        \"max\": 5.4175,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          1.666666667,\n          0.470588235,\n          4.705882353\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CA/FA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5357534124720503,\n        \"min\": 0.0,\n        \"max\": 3.347826087,\n        \"num_unique_values\": 134,\n        \"samples\": [\n          0.994623656,\n          1.093414716,\n          0.851764706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Strength\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.73083812387287,\n        \"min\": 0.26,\n        \"max\": 100.4,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          39.3,\n          41.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/Strength_ِDataset.csv')\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JfY2BoqMvIg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the features and target variable\n",
        "X = data.drop(['Strength'], axis=1)\n",
        "y = data['Strength']\n",
        "\n",
        "# One-hot encoding for categorical variables\n",
        "X = pd.get_dummies(X, columns=['Type'], drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol6qgMVdkp_-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "# Define a function to calculate performance metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    vaf = 1 - np.var(y_true - y_pred) / np.var(y_true)\n",
        "    rsr = np.sqrt(np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
        "    wmape = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true)) * 100\n",
        "    a20 = np.mean(np.abs((y_true - y_pred) / y_true) <= 0.2) * 100\n",
        "    return mse, r2, rmse, mae, mape, vaf, rsr, wmape, a20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iikI2WtX8LK6"
      },
      "source": [
        "RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jnUSG3H7LLj",
        "outputId": "4c3339ee-1be3-4859-bedc-a7748f191c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Metrics:\n",
            "MSE: 11.541201533742568\n",
            "R^2: 0.970887088495531\n",
            "RMSE: 3.3972343948780703\n",
            "MAE: 2.134923319615912\n",
            "MAPE: 6.19448086261275\n",
            "VAF: 0.9710415567161257\n",
            "RSR: 0.17062506118524629\n",
            "WMAPE: 4.97213181696579\n",
            "A-20: 93.4156378600823\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 69.90453134845173\n",
            "R^2: 0.8708702335056081\n",
            "RMSE: 8.360892975541054\n",
            "MAE: 4.958761748633878\n",
            "MAPE: 11.388849281107696\n",
            "VAF: 0.8767643891054007\n",
            "RSR: 0.3593463044117636\n",
            "WMAPE: 10.517094430162091\n",
            "A-20: 85.24590163934425\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Train the RandomForest model with the best configuration\n",
        "model = RandomForestRegressor(n_estimators=5, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the values for both train and test sets\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate metrics for train and test sets\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "# Output the results\n",
        "metric_names = [\"MSE\", \"R^2\", \"RMSE\", \"MAE\", \"MAPE\", \"VAF\", \"RSR\", \"WMAPE\", \"A-20\"]\n",
        "\n",
        "print(\"Train Metrics:\")\n",
        "for name, value in zip(metric_names, train_metrics):\n",
        "    print(f\"{name}: {value}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "for name, value in zip(metric_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX4mHaIiz-5s"
      },
      "source": [
        "\n",
        "\n",
        "DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KW5HhspLZNBi",
        "outputId": "a1e25783-d965-4765-c957-0a787899c624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Metrics:\n",
            "MSE: 14.035835376792427\n",
            "R^2: 0.9645943247744895\n",
            "RMSE: 3.7464430299675486\n",
            "MAE: 2.5133167372676235\n",
            "MAPE: 7.113034422589276\n",
            "VAF: 0.9645943247744895\n",
            "RSR: 0.1881639583594865\n",
            "WMAPE: 5.853391548380866\n",
            "A-20: 93.82716049382715\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 91.04153117838217\n",
            "R^2: 0.8318253275491451\n",
            "RMSE: 9.541568591085126\n",
            "MAE: 5.419707916400539\n",
            "MAPE: 12.572510903096084\n",
            "VAF: 0.8382206837804711\n",
            "RSR: 0.4100910538537203\n",
            "WMAPE: 11.494720422166822\n",
            "A-20: 85.24590163934425\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Create and fit the model with optimized parameters\n",
        "model = DecisionTreeRegressor(random_state=42, max_depth=7, min_samples_split=2, min_samples_leaf=1, max_features=None)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate metrics for train and test sets\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "# Output the results\n",
        "metric_names = [\"MSE\", \"R^2\", \"RMSE\", \"MAE\", \"MAPE\", \"VAF\", \"RSR\", \"WMAPE\", \"A-20\"]\n",
        "\n",
        "print(\"Train Metrics:\")\n",
        "for name, value in zip(metric_names, train_metrics):\n",
        "    print(f\"{name}: {value}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "for name, value in zip(metric_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou2yVpVY1EgX"
      },
      "source": [
        "Gradient Boosted Regression Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3_4NqxWD-vX",
        "outputId": "98fda1ed-5261-4346-dda3-e55622006d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Metrics:\n",
            "MSE: 0.47442964883192823\n",
            "R^2: 0.9988032417299743\n",
            "RMSE: 0.6887885370938807\n",
            "MAE: 0.4851545415231243\n",
            "MAPE: 3.636125981165375\n",
            "VAF: 0.9988032417299743\n",
            "RSR: 0.03459419416644571\n",
            "WMAPE: 1.1299011584577934\n",
            "A-20: 97.94238683127571\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 39.76524498163991\n",
            "R^2: 0.9265444356750533\n",
            "RMSE: 6.305968996247913\n",
            "MAE: 4.096905468028911\n",
            "MAPE: 13.849079590823479\n",
            "VAF: 0.9299811009540419\n",
            "RSR: 0.271026870116132\n",
            "WMAPE: 8.68917360076393\n",
            "A-20: 81.9672131147541\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create the Gradient Boosting Regressor model with best parameters\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=200, learning_rate=0.2, max_depth=3, min_samples_split=5, random_state=42)\n",
        "\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict with train and test data\n",
        "    y_train_pred = gb_regressor.predict(X_train)\n",
        "    y_test_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for train and test sets\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "    # Output the results\n",
        "    metric_names = [\"MSE\", \"R^2\", \"RMSE\", \"MAE\", \"MAPE\", \"VAF\", \"RSR\", \"WMAPE\", \"A-20\"]\n",
        "\n",
        "    print(\"Train Metrics:\")\n",
        "    for name, value in zip(metric_names, train_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    for name, value in zip(metric_names, test_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in training or predicting with Gradient Boosting Regressor:\")\n",
        "    print(f\"Exception: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdlz_vOHeo2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5858eca-a3f7-4485-cf33-aafdcc56c20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install catboost -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyuxiB6o2oc-"
      },
      "source": [
        "CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPk3hZvWEgz9",
        "outputId": "325ca8d0-119d-421e-b52b-242f9720427f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 18.6647238\ttotal: 47ms\tremaining: 46.9s\n",
            "100:\tlearn: 2.7710829\ttotal: 104ms\tremaining: 929ms\n",
            "200:\tlearn: 1.6246396\ttotal: 159ms\tremaining: 630ms\n",
            "300:\tlearn: 1.1054132\ttotal: 215ms\tremaining: 499ms\n",
            "400:\tlearn: 0.7529452\ttotal: 274ms\tremaining: 409ms\n",
            "500:\tlearn: 0.5328165\ttotal: 357ms\tremaining: 355ms\n",
            "600:\tlearn: 0.4012383\ttotal: 411ms\tremaining: 273ms\n",
            "700:\tlearn: 0.3167058\ttotal: 469ms\tremaining: 200ms\n",
            "800:\tlearn: 0.2544292\ttotal: 527ms\tremaining: 131ms\n",
            "900:\tlearn: 0.2018419\ttotal: 589ms\tremaining: 64.7ms\n",
            "999:\tlearn: 0.1636592\ttotal: 643ms\tremaining: 0us\n",
            "Train Metrics:\n",
            "MSE: 0.02678435410400192\n",
            "R^2: 0.9999324359315229\n",
            "RMSE: 0.1636592622004692\n",
            "MAE: 0.11977594059287813\n",
            "MAPE: 0.9083796936746288\n",
            "VAF: 0.9999324359345835\n",
            "RSR: 0.008219736521150331\n",
            "WMAPE: 0.2789522975635471\n",
            "A-20: 99.1769547325103\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 28.816803136572045\n",
            "R^2: 0.9467687288883766\n",
            "RMSE: 5.368128457532666\n",
            "MAE: 3.6679693510840217\n",
            "MAPE: 16.38994350588471\n",
            "VAF: 0.9482929607128239\n",
            "RSR: 0.23071903066635693\n",
            "WMAPE: 7.77943809115626\n",
            "A-20: 85.24590163934425\n"
          ]
        }
      ],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "\n",
        "\n",
        "# Create the CatBoost Regressor model\n",
        "catboost_model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, random_seed=42, verbose=100)\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    catboost_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict with train and test data\n",
        "    y_train_pred = catboost_model.predict(X_train)\n",
        "    y_test_pred = catboost_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics for train and test sets\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "    # Output the results\n",
        "    metric_names = [\"MSE\", \"R^2\", \"RMSE\", \"MAE\", \"MAPE\", \"VAF\", \"RSR\", \"WMAPE\", \"A-20\"]\n",
        "\n",
        "    print(\"Train Metrics:\")\n",
        "    for name, value in zip(metric_names, train_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    for name, value in zip(metric_names, test_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in training or predicting with CatBoost Regressor:\")\n",
        "    print(f\"Exception: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnCi4FTI4l-J"
      },
      "source": [
        "GaussianProcessRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d22TXybhsCeq",
        "outputId": "303ae77f-dd40-4afa-bb83-97435d5da8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Metrics:\n",
            "Mean Squared Error (MSE): 0.702093482378787\n",
            "R^2 Score: 0.9982289551602925\n",
            "Root Mean Squared Error (RMSE): 0.8379101875372963\n",
            "Mean Absolute Error (MAE): 0.5052407469003674\n",
            "Mean Absolute Percentage Error (MAPE): 2.8363568909572976\n",
            "Variance Accounted For (VAF): 0.9982289693958333\n",
            "Residual Standard Deviation (RSR): 0.04208378357167305\n",
            "Weighted Mean Absolute Percentage Error (WMAPE): 1.1766809467156063\n",
            "A-20 Index: 97.53086419753086\n",
            "\n",
            "Test Metrics:\n",
            "Mean Squared Error (MSE): 33.621509976073476\n",
            "R^2 Score: 0.9378933289637826\n",
            "Root Mean Squared Error (RMSE): 5.798405813331236\n",
            "Mean Absolute Error (MAE): 3.50440460849839\n",
            "Mean Absolute Percentage Error (MAPE): 12.8350944726198\n",
            "Variance Accounted For (VAF): 0.9431638383786489\n",
            "Residual Standard Deviation (RSR): 0.24921210050119427\n",
            "Weighted Mean Absolute Percentage Error (WMAPE): 7.432531760419124\n",
            "A-20 Index: 86.88524590163934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__constant_value is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Create a pipeline with StandardScaler and GaussianProcessRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('gpr', GaussianProcessRegressor(\n",
        "        kernel=C(31.1**2, (1e-3, 1e3)) * Matern(length_scale=3.81, length_scale_bounds=(1e-2, 1e2), nu=1.5) + WhiteKernel(noise_level=0.753),\n",
        "        n_restarts_optimizer=20\n",
        "    ))\n",
        "])\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions for training and testing data\n",
        "    y_train_pred, _ = pipeline.predict(X_train, return_std=True)\n",
        "    y_test_pred, _ = pipeline.predict(X_test, return_std=True)\n",
        "\n",
        "    # Calculate metrics for training and testing data\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "    # Display results\n",
        "    metric_names = [\"Mean Squared Error (MSE)\", \"R^2 Score\", \"Root Mean Squared Error (RMSE)\",\n",
        "                    \"Mean Absolute Error (MAE)\", \"Mean Absolute Percentage Error (MAPE)\",\n",
        "                    \"Variance Accounted For (VAF)\", \"Residual Standard Deviation (RSR)\",\n",
        "                    \"Weighted Mean Absolute Percentage Error (WMAPE)\", \"A-20 Index\"]\n",
        "\n",
        "    print(\"Train Metrics:\")\n",
        "    for name, value in zip(metric_names, train_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    for name, value in zip(metric_names, test_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in training or predicting with Gaussian Process Regressor:\")\n",
        "    print(f\"Exception: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTB39-cZ6iaL"
      },
      "source": [
        "LGBMRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFJReokqtgD9",
        "outputId": "074ba5d3-a607-4fe6-e24d-c19a724ec7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'subsample': 0.8, 'num_leaves': 31, 'n_estimators': 500, 'min_gain_to_split': 0.01, 'min_child_samples': 10, 'max_depth': -1, 'learning_rate': 0.1}\n",
            "Best R^2 score from RandomizedSearchCV: 0.8889641030531177\n",
            "\n",
            "Train Metrics:\n",
            "Mean Squared Error (MSE): 0.9056927223680146\n",
            "R^2 Score: 0.997715372008759\n",
            "Root Mean Squared Error (RMSE): 0.9516788966705181\n",
            "Mean Absolute Error (MAE): 0.3368531299535459\n",
            "Mean Absolute Percentage Error (MAPE): 17.084215370940193\n",
            "Variance Accounted For (VAF): 0.997715372008759\n",
            "Residual Standard Deviation (RSR): 0.04779778228371045\n",
            "Weighted Mean Absolute Percentage Error (WMAPE): 0.7845144365128112\n",
            "A-20 Index: 95.88477366255144\n",
            "\n",
            "Test Metrics:\n",
            "Mean Squared Error (MSE): 54.265654187810284\n",
            "R^2 Score: 0.899758840825237\n",
            "Root Mean Squared Error (RMSE): 7.36652253019091\n",
            "Mean Absolute Error (MAE): 3.856540234678136\n",
            "Mean Absolute Percentage Error (MAPE): 37.51529183761961\n",
            "Variance Accounted For (VAF): 0.901006086540506\n",
            "Residual Standard Deviation (RSR): 0.3166088425403861\n",
            "Weighted Mean Absolute Percentage Error (WMAPE): 8.179380231970905\n",
            "A-20 Index: 90.1639344262295\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lightgbm import LGBMRegressor\n",
        "import logging\n",
        "import warnings\n",
        "logging.getLogger('sklearn').setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.deprecation\")\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the parameter grid for randomized search\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 500, 1000],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'num_leaves': [31, 50, 70, 100],\n",
        "    'max_depth': [-1, 10, 15, 20],\n",
        "    'min_child_samples':[10, 20],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'min_gain_to_split': [0, 0.01, 0.1],\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Initialize LGBMRegressor model\n",
        "    lgbm_model = LGBMRegressor(random_state=42, verbose=-1)\n",
        "    # Perform randomized search to find the best parameters\n",
        "    random_search = RandomizedSearchCV(lgbm_model, param_distributions=param_dist, cv=5, n_iter=50, scoring='r2', n_jobs=-1, random_state=42)\n",
        "    random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Display the best parameters and model performance\n",
        "    print(\"Best parameters found:\", random_search.best_params_)\n",
        "    print(\"Best R^2 score from RandomizedSearchCV:\", random_search.best_score_)\n",
        "\n",
        "    best_lgbm_model = random_search.best_estimator_\n",
        "\n",
        "    # Make predictions for training and testing data\n",
        "    y_train_pred = best_lgbm_model.predict(X_train_scaled)\n",
        "    y_test_pred = best_lgbm_model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate metrics for training and testing data\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred)\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred)\n",
        "\n",
        "    # Display results\n",
        "    metric_names = [\"Mean Squared Error (MSE)\", \"R^2 Score\", \"Root Mean Squared Error (RMSE)\",\n",
        "                    \"Mean Absolute Error (MAE)\", \"Mean Absolute Percentage Error (MAPE)\",\n",
        "                    \"Variance Accounted For (VAF)\", \"Residual Standard Deviation (RSR)\",\n",
        "                    \"Weighted Mean Absolute Percentage Error (WMAPE)\", \"A-20 Index\"]\n",
        "\n",
        "    print(\"\\nTrain Metrics:\")\n",
        "    for name, value in zip(metric_names, train_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "    print(\"\\nTest Metrics:\")\n",
        "    for name, value in zip(metric_names, test_metrics):\n",
        "        print(f\"{name}: {value}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error in training or predicting with LGBM Regressor:\")\n",
        "    print(f\"Exception: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhG9GA9Ju_Rq"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv1y_rym0szU"
      },
      "source": [
        "`ANN`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model for KerasRegressor\n",
        "def create_nn_model():\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # افزایش نورون‌ها و تغییر به gelu\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Use Nadam optimizer with a variable learning rate\n",
        "    model.compile(optimizer=Nadam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Initialize the neural network model\n",
        "nn_model = KerasRegressor(model=create_nn_model, epochs=1000, batch_size=128, verbose=1)\n",
        "\n",
        "# Use EarlyStopping and ReduceLROnPlateau to prevent overfitting and adjust learning rate\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
        "\n",
        "# Train the model with EarlyStopping and ReduceLROnPlateau\n",
        "nn_model.fit(X_train_scaled, y_train, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Make predictions with the neural network on training and testing data\n",
        "nn_train_pred = nn_model.predict(X_train_scaled)\n",
        "nn_test_pred = nn_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics for training and testing data\n",
        "train_metrics = calculate_metrics(y_train, nn_train_pred)\n",
        "test_metrics = calculate_metrics(y_test, nn_test_pred)\n",
        "\n",
        "# Display results\n",
        "metrics_names = [\"MSE\", \"R^2\", \"RMSE\", \"MAE\", \"MAPE\", \"VAF\", \"RSR\", \"WMAPE\", \"A-20\"]\n",
        "\n",
        "print(\"Train Metrics:\")\n",
        "for name, value in zip(metrics_names, train_metrics):\n",
        "    print(f\"{name}: {value}\")\n",
        "\n",
        "print(\"\\nTest Metrics:\")\n",
        "for name, value in zip(metrics_names, test_metrics):\n",
        "    print(f\"{name}: {value}\")"
      ],
      "metadata": {
        "id": "eqaiBe3bIrwU",
        "outputId": "41f34787-1b1b-422c-fc28-824cb1fbd649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 624ms/step - loss: 2160.1094 - val_loss: 2282.3347\n",
            "Epoch 2/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 2125.2083 - val_loss: 2281.1509\n",
            "Epoch 3/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 2169.7346 - val_loss: 2280.3235\n",
            "Epoch 4/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 2249.9014 - val_loss: 2278.4485\n",
            "Epoch 5/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 2134.7551 - val_loss: 2275.5811\n",
            "Epoch 6/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 2197.4678 - val_loss: 2273.4182\n",
            "Epoch 7/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 2084.3726 - val_loss: 2270.7317\n",
            "Epoch 8/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 2119.0955 - val_loss: 2267.0393\n",
            "Epoch 9/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - loss: 2159.9509 - val_loss: 2263.1387\n",
            "Epoch 10/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 2109.1511 - val_loss: 2258.9009\n",
            "Epoch 11/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - loss: 2101.5979 - val_loss: 2255.2439\n",
            "Epoch 12/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 2126.7400 - val_loss: 2250.8794\n",
            "Epoch 13/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 2057.1062 - val_loss: 2247.0449\n",
            "Epoch 14/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 2177.7041 - val_loss: 2242.3020\n",
            "Epoch 15/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 2025.5203 - val_loss: 2238.4590\n",
            "Epoch 16/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 2020.2820 - val_loss: 2233.9653\n",
            "Epoch 17/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1991.9666 - val_loss: 2229.4707\n",
            "Epoch 18/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - loss: 1966.2288 - val_loss: 2225.1970\n",
            "Epoch 19/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1978.7729 - val_loss: 2220.4890\n",
            "Epoch 20/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 2032.0526 - val_loss: 2214.2441\n",
            "Epoch 21/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1962.4783 - val_loss: 2208.8423\n",
            "Epoch 22/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 2007.0629 - val_loss: 2203.8237\n",
            "Epoch 23/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - loss: 2042.0452 - val_loss: 2198.0671\n",
            "Epoch 24/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 1973.6383 - val_loss: 2192.8218\n",
            "Epoch 25/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1971.8865 - val_loss: 2187.6973\n",
            "Epoch 26/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1927.1383 - val_loss: 2182.5347\n",
            "Epoch 27/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 1945.4857 - val_loss: 2177.1331\n",
            "Epoch 28/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: 1951.4594 - val_loss: 2171.9087\n",
            "Epoch 29/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 1960.9745 - val_loss: 2166.3701\n",
            "Epoch 30/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1911.6511 - val_loss: 2161.9575\n",
            "Epoch 31/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - loss: 1938.1337 - val_loss: 2157.2300\n",
            "Epoch 32/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1961.4878 - val_loss: 2151.5945\n",
            "Epoch 33/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - loss: 1926.5995 - val_loss: 2145.0493\n",
            "Epoch 34/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1884.8917 - val_loss: 2138.6633\n",
            "Epoch 35/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1928.4529 - val_loss: 2132.7419\n",
            "Epoch 36/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 1901.9899 - val_loss: 2127.0386\n",
            "Epoch 37/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 1871.5972 - val_loss: 2122.0054\n",
            "Epoch 38/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 1831.2841 - val_loss: 2117.6733\n",
            "Epoch 39/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1839.2500 - val_loss: 2113.3733\n",
            "Epoch 40/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1833.6332 - val_loss: 2108.9253\n",
            "Epoch 41/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1875.9615 - val_loss: 2103.5750\n",
            "Epoch 42/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1797.9651 - val_loss: 2099.3250\n",
            "Epoch 43/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1828.9381 - val_loss: 2094.6272\n",
            "Epoch 44/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1850.7096 - val_loss: 2088.1399\n",
            "Epoch 45/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1930.4865 - val_loss: 2080.5938\n",
            "Epoch 46/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1832.1710 - val_loss: 2072.0059\n",
            "Epoch 47/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 1847.9449 - val_loss: 2063.0471\n",
            "Epoch 48/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1811.1747 - val_loss: 2056.7976\n",
            "Epoch 49/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1802.9500 - val_loss: 2049.1001\n",
            "Epoch 50/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1808.8882 - val_loss: 2042.5759\n",
            "Epoch 51/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1801.6190 - val_loss: 2033.4761\n",
            "Epoch 52/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1727.9368 - val_loss: 2026.0905\n",
            "Epoch 53/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1794.0226 - val_loss: 2022.5754\n",
            "Epoch 54/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1725.2402 - val_loss: 2018.4609\n",
            "Epoch 55/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1722.8341 - val_loss: 2013.1465\n",
            "Epoch 56/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1747.4009 - val_loss: 2004.9183\n",
            "Epoch 57/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1775.2297 - val_loss: 1996.9689\n",
            "Epoch 58/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1720.3511 - val_loss: 1989.6465\n",
            "Epoch 59/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1727.3965 - val_loss: 1984.0811\n",
            "Epoch 60/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1809.8009 - val_loss: 1975.1583\n",
            "Epoch 61/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1702.8745 - val_loss: 1969.4475\n",
            "Epoch 62/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1739.6417 - val_loss: 1963.2957\n",
            "Epoch 63/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1658.9175 - val_loss: 1955.4434\n",
            "Epoch 64/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1669.5099 - val_loss: 1949.2740\n",
            "Epoch 65/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1680.3318 - val_loss: 1948.0144\n",
            "Epoch 66/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1685.4614 - val_loss: 1944.8866\n",
            "Epoch 67/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1696.3054 - val_loss: 1937.5120\n",
            "Epoch 68/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 1671.5499 - val_loss: 1928.6213\n",
            "Epoch 69/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1681.4774 - val_loss: 1925.4092\n",
            "Epoch 70/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 1668.3499 - val_loss: 1920.5760\n",
            "Epoch 71/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 1668.7823 - val_loss: 1912.4567\n",
            "Epoch 72/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 1670.4159 - val_loss: 1906.7198\n",
            "Epoch 73/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 1603.3560 - val_loss: 1901.3721\n",
            "Epoch 74/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - loss: 1593.4508 - val_loss: 1898.7880\n",
            "Epoch 75/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1597.9430 - val_loss: 1895.0991\n",
            "Epoch 76/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 1618.2440 - val_loss: 1889.4949\n",
            "Epoch 77/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1609.7767 - val_loss: 1887.9934\n",
            "Epoch 78/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 1590.2446 - val_loss: 1885.3635\n",
            "Epoch 79/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1624.4636 - val_loss: 1871.4375\n",
            "Epoch 80/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 1568.7142 - val_loss: 1867.7736\n",
            "Epoch 81/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 1575.3359 - val_loss: 1867.9813\n",
            "Epoch 82/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 1611.0966 - val_loss: 1861.2308\n",
            "Epoch 83/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - loss: 1607.0804 - val_loss: 1859.1965\n",
            "Epoch 84/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 1576.8708 - val_loss: 1852.6824\n",
            "Epoch 85/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1510.7119 - val_loss: 1846.4901\n",
            "Epoch 86/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1592.9877 - val_loss: 1844.0869\n",
            "Epoch 87/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - loss: 1588.1066 - val_loss: 1841.3385\n",
            "Epoch 88/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 1561.7316 - val_loss: 1832.1453\n",
            "Epoch 89/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 1512.0018 - val_loss: 1820.9572\n",
            "Epoch 90/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 1554.2100 - val_loss: 1805.0546\n",
            "Epoch 91/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 1571.0233 - val_loss: 1788.4696\n",
            "Epoch 92/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 1556.2725 - val_loss: 1778.2578\n",
            "Epoch 93/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 1508.2294 - val_loss: 1777.2603\n",
            "Epoch 94/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - loss: 1510.3152 - val_loss: 1782.3265\n",
            "Epoch 95/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1479.1609 - val_loss: 1793.5031\n",
            "Epoch 96/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - loss: 1501.6080 - val_loss: 1807.7719\n",
            "Epoch 97/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 1410.1794 - val_loss: 1824.6102\n",
            "Epoch 98/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 1481.6483 - val_loss: 1823.2212\n",
            "Epoch 99/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - loss: 1522.7743 - val_loss: 1820.4297\n",
            "Epoch 100/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 1516.1443 - val_loss: 1809.7672\n",
            "Epoch 101/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - loss: 1441.3086 - val_loss: 1798.0452\n",
            "Epoch 102/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 1391.3442 - val_loss: 1783.7449\n",
            "Epoch 103/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1451.1285 - val_loss: 1763.9926\n",
            "Epoch 104/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1461.2091 - val_loss: 1750.0348\n",
            "Epoch 105/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1454.3213 - val_loss: 1734.6964\n",
            "Epoch 106/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1429.9218 - val_loss: 1726.2806\n",
            "Epoch 107/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - loss: 1474.0287 - val_loss: 1719.4906\n",
            "Epoch 108/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - loss: 1425.5876 - val_loss: 1714.9031\n",
            "Epoch 109/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - loss: 1395.1449 - val_loss: 1708.5090\n",
            "Epoch 110/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1418.7255 - val_loss: 1697.5109\n",
            "Epoch 111/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 1409.2570 - val_loss: 1685.6454\n",
            "Epoch 112/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1403.8851 - val_loss: 1671.7889\n",
            "Epoch 113/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1366.9646 - val_loss: 1663.3728\n",
            "Epoch 114/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 1415.4777 - val_loss: 1652.7996\n",
            "Epoch 115/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 1347.6558 - val_loss: 1637.7166\n",
            "Epoch 116/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1368.3245 - val_loss: 1624.4054\n",
            "Epoch 117/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1342.2531 - val_loss: 1599.0477\n",
            "Epoch 118/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1341.1672 - val_loss: 1577.4579\n",
            "Epoch 119/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 1360.9932 - val_loss: 1555.9183\n",
            "Epoch 120/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 1313.6649 - val_loss: 1538.9172\n",
            "Epoch 121/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1335.9813 - val_loss: 1528.7708\n",
            "Epoch 122/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 1291.6396 - val_loss: 1529.8407\n",
            "Epoch 123/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1285.2129 - val_loss: 1536.4335\n",
            "Epoch 124/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - loss: 1362.8397 - val_loss: 1535.2363\n",
            "Epoch 125/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - loss: 1269.7721 - val_loss: 1526.4746\n",
            "Epoch 126/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - loss: 1262.2952 - val_loss: 1518.5686\n",
            "Epoch 127/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - loss: 1271.4867 - val_loss: 1514.7910\n",
            "Epoch 128/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 1288.8218 - val_loss: 1504.1765\n",
            "Epoch 129/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1208.9639 - val_loss: 1494.1578\n",
            "Epoch 130/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 1260.8151 - val_loss: 1478.9618\n",
            "Epoch 131/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - loss: 1227.6638 - val_loss: 1466.8660\n",
            "Epoch 132/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 1260.3691 - val_loss: 1459.9608\n",
            "Epoch 133/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 1228.6504 - val_loss: 1448.3890\n",
            "Epoch 134/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 1205.6038 - val_loss: 1441.4775\n",
            "Epoch 135/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1223.2023 - val_loss: 1431.2733\n",
            "Epoch 136/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 1220.0735 - val_loss: 1423.3252\n",
            "Epoch 137/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - loss: 1204.8712 - val_loss: 1414.8181\n",
            "Epoch 138/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1213.8018 - val_loss: 1410.7936\n",
            "Epoch 139/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1198.5304 - val_loss: 1399.2173\n",
            "Epoch 140/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1174.5712 - val_loss: 1382.2522\n",
            "Epoch 141/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1143.3019 - val_loss: 1349.6776\n",
            "Epoch 142/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1167.9581 - val_loss: 1324.0118\n",
            "Epoch 143/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1190.0677 - val_loss: 1302.1681\n",
            "Epoch 144/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1152.7219 - val_loss: 1278.0895\n",
            "Epoch 145/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1117.2509 - val_loss: 1270.4418\n",
            "Epoch 146/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1158.1722 - val_loss: 1263.8942\n",
            "Epoch 147/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1135.3572 - val_loss: 1255.2042\n",
            "Epoch 148/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1110.9523 - val_loss: 1246.0546\n",
            "Epoch 149/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1111.9974 - val_loss: 1223.1584\n",
            "Epoch 150/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1077.3557 - val_loss: 1209.9066\n",
            "Epoch 151/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1094.0143 - val_loss: 1202.4641\n",
            "Epoch 152/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1108.5162 - val_loss: 1192.3350\n",
            "Epoch 153/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1076.8458 - val_loss: 1176.7927\n",
            "Epoch 154/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1062.0922 - val_loss: 1173.9010\n",
            "Epoch 155/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1059.7476 - val_loss: 1180.8567\n",
            "Epoch 156/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1090.1775 - val_loss: 1183.7593\n",
            "Epoch 157/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1043.4984 - val_loss: 1184.7030\n",
            "Epoch 158/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1088.3098 - val_loss: 1178.0049\n",
            "Epoch 159/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1032.9438 - val_loss: 1162.7764\n",
            "Epoch 160/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1035.2700 - val_loss: 1150.8457\n",
            "Epoch 161/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1045.3136 - val_loss: 1131.4795\n",
            "Epoch 162/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1024.8315 - val_loss: 1111.0344\n",
            "Epoch 163/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1055.8931 - val_loss: 1102.7789\n",
            "Epoch 164/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1035.4250 - val_loss: 1093.2537\n",
            "Epoch 165/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 991.5281 - val_loss: 1090.5319\n",
            "Epoch 166/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1000.0691 - val_loss: 1067.4921\n",
            "Epoch 167/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1026.9546 - val_loss: 1052.9851\n",
            "Epoch 168/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 997.4438 - val_loss: 1032.8424\n",
            "Epoch 169/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 942.1591 - val_loss: 1013.3826\n",
            "Epoch 170/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 937.1852 - val_loss: 996.9949\n",
            "Epoch 171/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 975.9594 - val_loss: 989.6779\n",
            "Epoch 172/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 952.1230 - val_loss: 989.4091\n",
            "Epoch 173/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 900.1301 - val_loss: 987.3521\n",
            "Epoch 174/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 899.8641 - val_loss: 982.3825\n",
            "Epoch 175/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 895.4288 - val_loss: 978.7203\n",
            "Epoch 176/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 959.9918 - val_loss: 975.1536\n",
            "Epoch 177/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: 900.9739 - val_loss: 968.2130\n",
            "Epoch 178/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 917.2117 - val_loss: 962.5740\n",
            "Epoch 179/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 893.7514 - val_loss: 951.5621\n",
            "Epoch 180/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 862.9284 - val_loss: 947.2911\n",
            "Epoch 181/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 868.3336 - val_loss: 943.7656\n",
            "Epoch 182/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 885.4595 - val_loss: 932.2273\n",
            "Epoch 183/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 819.6459 - val_loss: 924.1470\n",
            "Epoch 184/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 874.2069 - val_loss: 914.0463\n",
            "Epoch 185/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 831.0823 - val_loss: 904.5136\n",
            "Epoch 186/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 800.2393 - val_loss: 888.6769\n",
            "Epoch 187/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 863.6237 - val_loss: 867.3918\n",
            "Epoch 188/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 784.5074 - val_loss: 847.1346\n",
            "Epoch 189/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 778.6688 - val_loss: 824.1567\n",
            "Epoch 190/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 779.2413 - val_loss: 814.0355\n",
            "Epoch 191/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 807.3926 - val_loss: 801.1589\n",
            "Epoch 192/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 749.2245 - val_loss: 797.4373\n",
            "Epoch 193/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 785.8116 - val_loss: 797.3376\n",
            "Epoch 194/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 779.4404 - val_loss: 793.2328\n",
            "Epoch 195/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 793.2792 - val_loss: 783.0847\n",
            "Epoch 196/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 793.4941 - val_loss: 772.9255\n",
            "Epoch 197/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 787.7308 - val_loss: 765.1825\n",
            "Epoch 198/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 738.5391 - val_loss: 754.3776\n",
            "Epoch 199/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 711.8128 - val_loss: 747.9869\n",
            "Epoch 200/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 747.3522 - val_loss: 739.0974\n",
            "Epoch 201/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 727.8832 - val_loss: 724.5705\n",
            "Epoch 202/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 697.8563 - val_loss: 704.3205\n",
            "Epoch 203/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 681.5322 - val_loss: 688.9391\n",
            "Epoch 204/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 716.2260 - val_loss: 674.2096\n",
            "Epoch 205/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 684.1223 - val_loss: 660.4493\n",
            "Epoch 206/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 677.9234 - val_loss: 647.3261\n",
            "Epoch 207/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 662.1270 - val_loss: 637.1253\n",
            "Epoch 208/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 661.5693 - val_loss: 627.4874\n",
            "Epoch 209/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 647.1949 - val_loss: 621.0170\n",
            "Epoch 210/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 659.3318 - val_loss: 614.4030\n",
            "Epoch 211/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 635.7066 - val_loss: 603.7007\n",
            "Epoch 212/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 616.5961 - val_loss: 589.3810\n",
            "Epoch 213/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 614.3630 - val_loss: 577.9092\n",
            "Epoch 214/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 622.0993 - val_loss: 569.3334\n",
            "Epoch 215/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 647.8875 - val_loss: 560.9686\n",
            "Epoch 216/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 630.7126 - val_loss: 554.2108\n",
            "Epoch 217/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 563.6216 - val_loss: 552.1841\n",
            "Epoch 218/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 569.4289 - val_loss: 552.5284\n",
            "Epoch 219/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 611.7688 - val_loss: 552.2119\n",
            "Epoch 220/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 581.9919 - val_loss: 553.8552\n",
            "Epoch 221/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 549.5870 - val_loss: 547.2691\n",
            "Epoch 222/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 574.5717 - val_loss: 537.9857\n",
            "Epoch 223/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 568.1508 - val_loss: 532.0311\n",
            "Epoch 224/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 522.6477 - val_loss: 528.3329\n",
            "Epoch 225/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 509.5054 - val_loss: 519.4899\n",
            "Epoch 226/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 541.1435 - val_loss: 507.6516\n",
            "Epoch 227/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 522.3173 - val_loss: 496.3531\n",
            "Epoch 228/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 527.3611 - val_loss: 481.6290\n",
            "Epoch 229/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 504.7706 - val_loss: 467.0875\n",
            "Epoch 230/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 510.7076 - val_loss: 457.1341\n",
            "Epoch 231/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 490.9677 - val_loss: 449.9902\n",
            "Epoch 232/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 516.6265 - val_loss: 442.1388\n",
            "Epoch 233/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 487.3206 - val_loss: 438.4687\n",
            "Epoch 234/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 476.4681 - val_loss: 430.4466\n",
            "Epoch 235/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 460.0049 - val_loss: 426.6424\n",
            "Epoch 236/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 452.4377 - val_loss: 421.7707\n",
            "Epoch 237/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 451.9751 - val_loss: 414.4513\n",
            "Epoch 238/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 436.6292 - val_loss: 404.6134\n",
            "Epoch 239/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 415.6804 - val_loss: 401.1833\n",
            "Epoch 240/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 445.8302 - val_loss: 397.9865\n",
            "Epoch 241/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 435.4691 - val_loss: 392.3870\n",
            "Epoch 242/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 386.9380 - val_loss: 387.5194\n",
            "Epoch 243/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 422.3774 - val_loss: 382.4630\n",
            "Epoch 244/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 393.6292 - val_loss: 380.4045\n",
            "Epoch 245/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 412.7083 - val_loss: 378.1855\n",
            "Epoch 246/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 422.7799 - val_loss: 371.6906\n",
            "Epoch 247/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 397.7755 - val_loss: 367.0847\n",
            "Epoch 248/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 364.0575 - val_loss: 363.9702\n",
            "Epoch 249/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 380.7293 - val_loss: 356.6547\n",
            "Epoch 250/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 385.9818 - val_loss: 352.9995\n",
            "Epoch 251/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 348.1188 - val_loss: 345.3840\n",
            "Epoch 252/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 363.0118 - val_loss: 338.7664\n",
            "Epoch 253/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 352.1092 - val_loss: 329.5500\n",
            "Epoch 254/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 345.5780 - val_loss: 322.5798\n",
            "Epoch 255/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 351.7947 - val_loss: 317.3105\n",
            "Epoch 256/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 337.3195 - val_loss: 310.0382\n",
            "Epoch 257/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 322.9228 - val_loss: 300.1185\n",
            "Epoch 258/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 312.5161 - val_loss: 289.9664\n",
            "Epoch 259/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 317.3915 - val_loss: 285.0412\n",
            "Epoch 260/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 336.1136 - val_loss: 280.1311\n",
            "Epoch 261/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 331.4472 - val_loss: 274.5941\n",
            "Epoch 262/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 276.7808 - val_loss: 271.3946\n",
            "Epoch 263/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 329.5174 - val_loss: 269.8426\n",
            "Epoch 264/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 279.1968 - val_loss: 267.3194\n",
            "Epoch 265/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 291.6658 - val_loss: 264.1869\n",
            "Epoch 266/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 276.6183 - val_loss: 262.5405\n",
            "Epoch 267/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 308.4137 - val_loss: 261.8572\n",
            "Epoch 268/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 315.3651 - val_loss: 256.0171\n",
            "Epoch 269/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 266.4795 - val_loss: 248.6602\n",
            "Epoch 270/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 256.5193 - val_loss: 243.3671\n",
            "Epoch 271/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 262.7158 - val_loss: 237.9679\n",
            "Epoch 272/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 282.0309 - val_loss: 227.1134\n",
            "Epoch 273/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 272.2183 - val_loss: 221.1193\n",
            "Epoch 274/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 260.1535 - val_loss: 217.6347\n",
            "Epoch 275/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 240.1369 - val_loss: 216.9320\n",
            "Epoch 276/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 242.5430 - val_loss: 215.4947\n",
            "Epoch 277/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 239.9982 - val_loss: 213.0887\n",
            "Epoch 278/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 255.9072 - val_loss: 205.5075\n",
            "Epoch 279/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 231.3421 - val_loss: 200.8796\n",
            "Epoch 280/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 233.1699 - val_loss: 196.6822\n",
            "Epoch 281/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 234.4962 - val_loss: 194.8002\n",
            "Epoch 282/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 231.0958 - val_loss: 191.1271\n",
            "Epoch 283/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 214.0283 - val_loss: 191.6433\n",
            "Epoch 284/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 223.3309 - val_loss: 189.1622\n",
            "Epoch 285/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 231.4537 - val_loss: 186.3992\n",
            "Epoch 286/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 212.1816 - val_loss: 184.0173\n",
            "Epoch 287/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 226.8984 - val_loss: 181.0175\n",
            "Epoch 288/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 190.3987 - val_loss: 176.3260\n",
            "Epoch 289/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 219.8700 - val_loss: 170.5215\n",
            "Epoch 290/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 189.7366 - val_loss: 165.0292\n",
            "Epoch 291/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 186.6249 - val_loss: 160.8672\n",
            "Epoch 292/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 222.5482 - val_loss: 159.8482\n",
            "Epoch 293/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 193.2092 - val_loss: 160.9807\n",
            "Epoch 294/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 197.9558 - val_loss: 164.1873\n",
            "Epoch 295/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 172.0363 - val_loss: 161.2591\n",
            "Epoch 296/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 216.9322 - val_loss: 159.3371\n",
            "Epoch 297/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 161.9156 - val_loss: 153.9615\n",
            "Epoch 298/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 172.7332 - val_loss: 147.3110\n",
            "Epoch 299/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 175.9206 - val_loss: 139.9006\n",
            "Epoch 300/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 164.3282 - val_loss: 132.9359\n",
            "Epoch 301/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 168.1257 - val_loss: 127.9136\n",
            "Epoch 302/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 169.1893 - val_loss: 123.0610\n",
            "Epoch 303/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 160.9573 - val_loss: 119.6729\n",
            "Epoch 304/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 151.2942 - val_loss: 116.3757\n",
            "Epoch 305/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 148.6731 - val_loss: 113.2745\n",
            "Epoch 306/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 134.8297 - val_loss: 112.0621\n",
            "Epoch 307/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 138.5363 - val_loss: 113.2845\n",
            "Epoch 308/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 140.4852 - val_loss: 113.4056\n",
            "Epoch 309/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 131.0696 - val_loss: 112.7087\n",
            "Epoch 310/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 130.7011 - val_loss: 111.3562\n",
            "Epoch 311/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 134.0271 - val_loss: 108.0999\n",
            "Epoch 312/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 121.4188 - val_loss: 105.3876\n",
            "Epoch 313/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 147.2946 - val_loss: 103.4854\n",
            "Epoch 314/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 138.4675 - val_loss: 100.4882\n",
            "Epoch 315/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 127.5641 - val_loss: 98.0388\n",
            "Epoch 316/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 121.2593 - val_loss: 95.9709\n",
            "Epoch 317/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 115.4151 - val_loss: 94.8458\n",
            "Epoch 318/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 130.0170 - val_loss: 92.5928\n",
            "Epoch 319/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 120.6906 - val_loss: 89.9346\n",
            "Epoch 320/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 104.0683 - val_loss: 86.2464\n",
            "Epoch 321/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 112.2963 - val_loss: 82.7856\n",
            "Epoch 322/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 105.7122 - val_loss: 81.0709\n",
            "Epoch 323/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 110.0509 - val_loss: 80.7821\n",
            "Epoch 324/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 106.8979 - val_loss: 81.0362\n",
            "Epoch 325/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 118.3064 - val_loss: 82.0188\n",
            "Epoch 326/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 118.7317 - val_loss: 81.8587\n",
            "Epoch 327/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 108.3617 - val_loss: 81.1661\n",
            "Epoch 328/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 104.8340 - val_loss: 77.5803\n",
            "Epoch 329/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 122.9357 - val_loss: 75.6181\n",
            "Epoch 330/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 95.4744 - val_loss: 73.0491\n",
            "Epoch 331/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 107.8560 - val_loss: 71.5648\n",
            "Epoch 332/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 87.8342 - val_loss: 70.6685\n",
            "Epoch 333/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 80.8555 - val_loss: 70.2069\n",
            "Epoch 334/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 111.9459 - val_loss: 68.9265\n",
            "Epoch 335/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 91.9459 - val_loss: 67.7297\n",
            "Epoch 336/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 110.7395 - val_loss: 67.4400\n",
            "Epoch 337/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 82.0051 - val_loss: 68.1974\n",
            "Epoch 338/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 85.5851 - val_loss: 69.8616\n",
            "Epoch 339/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 86.7884 - val_loss: 70.0875\n",
            "Epoch 340/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 76.2529 - val_loss: 68.3378\n",
            "Epoch 341/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 94.7162 - val_loss: 67.1453\n",
            "Epoch 342/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 96.9899 - val_loss: 66.0943\n",
            "Epoch 343/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 70.3516 - val_loss: 66.3975\n",
            "Epoch 344/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 69.1161 - val_loss: 65.5245\n",
            "Epoch 345/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 80.4884 - val_loss: 65.7321\n",
            "Epoch 346/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 75.5231 - val_loss: 65.4336\n",
            "Epoch 347/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 78.1769 - val_loss: 64.6281\n",
            "Epoch 348/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 71.0785 - val_loss: 63.4496\n",
            "Epoch 349/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 78.5670 - val_loss: 61.5042\n",
            "Epoch 350/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 77.2182 - val_loss: 58.4471\n",
            "Epoch 351/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 70.5485 - val_loss: 53.7990\n",
            "Epoch 352/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 74.9407 - val_loss: 49.8459\n",
            "Epoch 353/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 78.7692 - val_loss: 47.2271\n",
            "Epoch 354/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - loss: 71.8501 - val_loss: 44.6805\n",
            "Epoch 355/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - loss: 74.0349 - val_loss: 42.1780\n",
            "Epoch 356/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 86.8136 - val_loss: 40.2328\n",
            "Epoch 357/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 66.2002 - val_loss: 40.4739\n",
            "Epoch 358/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 66.2211 - val_loss: 40.6783\n",
            "Epoch 359/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 64.1528 - val_loss: 41.8054\n",
            "Epoch 360/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 60.7323 - val_loss: 44.7698\n",
            "Epoch 361/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 212ms/step - loss: 67.6567 - val_loss: 46.9034\n",
            "Epoch 362/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 66.3155 - val_loss: 48.9572\n",
            "Epoch 363/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 65.2519 - val_loss: 49.0123\n",
            "Epoch 364/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 61.6801 - val_loss: 49.5088\n",
            "Epoch 365/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 63.7173 - val_loss: 49.3328\n",
            "Epoch 366/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 52.6729 - val_loss: 48.6804\n",
            "Epoch 367/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 77.1675 - val_loss: 48.5746\n",
            "Epoch 368/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 59.9198 - val_loss: 46.9073\n",
            "Epoch 369/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 59.0290 - val_loss: 44.7061\n",
            "Epoch 370/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 62.7446 - val_loss: 42.9982\n",
            "Epoch 371/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 48.6711 - val_loss: 41.1476\n",
            "Epoch 372/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 55.0252 - val_loss: 39.8188\n",
            "Epoch 373/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 49.8278 - val_loss: 39.3645\n",
            "Epoch 374/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 59.8853 - val_loss: 38.8967\n",
            "Epoch 375/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 63.9072 - val_loss: 38.1636\n",
            "Epoch 376/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 61.1072 - val_loss: 37.4247\n",
            "Epoch 377/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 53.5246 - val_loss: 38.1904\n",
            "Epoch 378/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 57.3739 - val_loss: 40.1585\n",
            "Epoch 379/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 53.3558 - val_loss: 42.0644\n",
            "Epoch 380/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 58.1680 - val_loss: 43.0657\n",
            "Epoch 381/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 64.6301 - val_loss: 41.9501\n",
            "Epoch 382/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 57.9142 - val_loss: 40.4434\n",
            "Epoch 383/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 45.9229 - val_loss: 40.0958\n",
            "Epoch 384/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 63.7724 - val_loss: 39.9136\n",
            "Epoch 385/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 57.7164 - val_loss: 39.6364\n",
            "Epoch 386/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 59.8669 - val_loss: 38.1827\n",
            "Epoch 387/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 55.8011 - val_loss: 36.1999\n",
            "Epoch 388/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 52.9348 - val_loss: 33.7564\n",
            "Epoch 389/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 54.8707 - val_loss: 32.7380\n",
            "Epoch 390/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 49.9057 - val_loss: 32.3693\n",
            "Epoch 391/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 60.8401 - val_loss: 32.4707\n",
            "Epoch 392/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 47.7522 - val_loss: 33.2111\n",
            "Epoch 393/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 53.3098 - val_loss: 32.4167\n",
            "Epoch 394/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 45.7074 - val_loss: 31.8277\n",
            "Epoch 395/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 46.1022 - val_loss: 31.6945\n",
            "Epoch 396/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 45.9178 - val_loss: 31.2085\n",
            "Epoch 397/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 47.8701 - val_loss: 30.8599\n",
            "Epoch 398/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 62.7133 - val_loss: 30.9459\n",
            "Epoch 399/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 46.6782 - val_loss: 31.6985\n",
            "Epoch 400/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 49.3739 - val_loss: 34.0751\n",
            "Epoch 401/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 46.8009 - val_loss: 36.0238\n",
            "Epoch 402/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 68.9907 - val_loss: 35.4549\n",
            "Epoch 403/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 58.9791 - val_loss: 35.1507\n",
            "Epoch 404/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 51.0156 - val_loss: 35.9105\n",
            "Epoch 405/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 52.3684 - val_loss: 35.7615\n",
            "Epoch 406/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 43.6317 - val_loss: 35.6223\n",
            "Epoch 407/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 46.2693 - val_loss: 35.1133\n",
            "Epoch 408/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 56.6825 - val_loss: 36.1351\n",
            "Epoch 409/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 45.7610 - val_loss: 38.0355\n",
            "Epoch 410/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 55.5531 - val_loss: 39.3557\n",
            "Epoch 411/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 39.0377 - val_loss: 40.3886\n",
            "Epoch 412/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 42.3525 - val_loss: 39.8941\n",
            "Epoch 413/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 48.4032 - val_loss: 41.2826\n",
            "Epoch 414/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 44.8561 - val_loss: 41.8705\n",
            "Epoch 415/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 45.3767 - val_loss: 42.5748\n",
            "Epoch 416/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 45.6874 - val_loss: 43.2571\n",
            "Epoch 417/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 48.1656 - val_loss: 41.8240\n",
            "Epoch 418/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 42.1086 - val_loss: 40.3078\n",
            "Epoch 419/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 48.0061 - val_loss: 37.3832\n",
            "Epoch 420/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 38.9795 - val_loss: 34.1360\n",
            "Epoch 421/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 36.1716 - val_loss: 32.4696\n",
            "Epoch 422/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 45.1238 - val_loss: 31.0725\n",
            "Epoch 423/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 34.8847 - val_loss: 30.0051\n",
            "Epoch 424/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 59.6848 - val_loss: 28.7748\n",
            "Epoch 425/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 52.6414 - val_loss: 28.7220\n",
            "Epoch 426/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 58.8016 - val_loss: 28.2604\n",
            "Epoch 427/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 55.7146 - val_loss: 27.9779\n",
            "Epoch 428/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 42.4963 - val_loss: 26.7810\n",
            "Epoch 429/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 41.2337 - val_loss: 25.2634\n",
            "Epoch 430/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 40.8318 - val_loss: 25.2669\n",
            "Epoch 431/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 41.4673 - val_loss: 26.1248\n",
            "Epoch 432/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 30.4876 - val_loss: 26.8910\n",
            "Epoch 433/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 47.0844 - val_loss: 27.5622\n",
            "Epoch 434/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 39.8188 - val_loss: 29.4754\n",
            "Epoch 435/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 43.4968 - val_loss: 29.7219\n",
            "Epoch 436/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 33.4185 - val_loss: 30.0783\n",
            "Epoch 437/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 51.2445 - val_loss: 30.1167\n",
            "Epoch 438/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 39.6361 - val_loss: 29.7047\n",
            "Epoch 439/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 50.2057 - val_loss: 28.2074\n",
            "Epoch 440/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 49.3175 - val_loss: 27.3552\n",
            "Epoch 441/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 48.4532 - val_loss: 26.5262\n",
            "Epoch 442/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 54.3419 - val_loss: 25.7148\n",
            "Epoch 443/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 46.9466 - val_loss: 25.2916\n",
            "Epoch 444/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 43.7548 - val_loss: 24.5981\n",
            "Epoch 445/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 39.3121 - val_loss: 23.8204\n",
            "Epoch 446/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 48.3581 - val_loss: 24.0644\n",
            "Epoch 447/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 53.6231 - val_loss: 23.9806\n",
            "Epoch 448/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 50.1317 - val_loss: 24.4797\n",
            "Epoch 449/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 43.7401 - val_loss: 24.2373\n",
            "Epoch 450/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 40.1478 - val_loss: 23.9881\n",
            "Epoch 451/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 39.1715 - val_loss: 24.8321\n",
            "Epoch 452/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 44.2813 - val_loss: 25.0756\n",
            "Epoch 453/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 45.6645 - val_loss: 26.4668\n",
            "Epoch 454/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 45.5233 - val_loss: 27.1473\n",
            "Epoch 455/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 35.9570 - val_loss: 27.8340\n",
            "Epoch 456/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 55.8983 - val_loss: 28.6246\n",
            "Epoch 457/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 49.5889 - val_loss: 29.3220\n",
            "Epoch 458/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 33.7296 - val_loss: 29.5633\n",
            "Epoch 459/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 49.7667 - val_loss: 30.1305\n",
            "Epoch 460/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 47.8386 - val_loss: 29.7138\n",
            "Epoch 461/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 47.7458 - val_loss: 30.5354\n",
            "Epoch 462/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 58.1689 - val_loss: 30.1802\n",
            "Epoch 463/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 42.2459 - val_loss: 30.2326\n",
            "Epoch 464/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 45.2388 - val_loss: 30.1565\n",
            "Epoch 465/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 36.4524 - val_loss: 29.8483\n",
            "Epoch 466/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 43.5855 - val_loss: 28.6360\n",
            "Epoch 467/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 56.7435 - val_loss: 26.6074\n",
            "Epoch 468/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 48.5999 - val_loss: 24.6397\n",
            "Epoch 469/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 39.4887 - val_loss: 24.1811\n",
            "Epoch 470/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 44.0893 - val_loss: 23.3755\n",
            "Epoch 471/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 33.4396 - val_loss: 23.6053\n",
            "Epoch 472/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 42.5460 - val_loss: 24.1698\n",
            "Epoch 473/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 38.4021 - val_loss: 24.1988\n",
            "Epoch 474/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 40.1279 - val_loss: 23.7821\n",
            "Epoch 475/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 42.6826 - val_loss: 23.6369\n",
            "Epoch 476/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 39.5544 - val_loss: 23.9369\n",
            "Epoch 477/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 36.2611 - val_loss: 24.1714\n",
            "Epoch 478/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 35.1083 - val_loss: 25.0350\n",
            "Epoch 479/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 49.1060 - val_loss: 25.5948\n",
            "Epoch 480/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 35.4698 - val_loss: 26.7433\n",
            "Epoch 481/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 52.0282 - val_loss: 28.8286\n",
            "Epoch 482/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 47.5410 - val_loss: 29.9811\n",
            "Epoch 483/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 42.4501 - val_loss: 30.3579\n",
            "Epoch 484/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 41.7093 - val_loss: 30.5839\n",
            "Epoch 485/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.1613 - val_loss: 31.5984\n",
            "Epoch 486/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 40.5140 - val_loss: 32.5819\n",
            "Epoch 487/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 46.6438 - val_loss: 32.6841\n",
            "Epoch 488/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 42.3168 - val_loss: 32.4683\n",
            "Epoch 489/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 39.9336 - val_loss: 32.0179\n",
            "Epoch 490/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 41.8311 - val_loss: 31.2952\n",
            "Epoch 491/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 32.6185 - val_loss: 30.1485\n",
            "Epoch 492/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 40.9212 - val_loss: 28.5743\n",
            "Epoch 493/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 42.3198 - val_loss: 28.2611\n",
            "Epoch 494/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 43.1154 - val_loss: 28.0309\n",
            "Epoch 495/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 33.2942 - val_loss: 28.7774\n",
            "Epoch 496/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 30.8258 - val_loss: 30.2603\n",
            "Epoch 497/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 40.4899 - val_loss: 31.7373\n",
            "Epoch 498/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 47.7901 - val_loss: 33.7348\n",
            "Epoch 499/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 40.7989 - val_loss: 34.2220\n",
            "Epoch 500/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 36.1010 - val_loss: 33.7934\n",
            "Epoch 501/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 50.8172 - val_loss: 35.6303\n",
            "Epoch 502/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 41.2923 - val_loss: 36.2926\n",
            "Epoch 503/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 52.9004 - val_loss: 37.2471\n",
            "Epoch 504/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 41.1135 - val_loss: 37.7988\n",
            "Epoch 505/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 42.3830 - val_loss: 37.0821\n",
            "Epoch 506/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 39.4499 - val_loss: 36.5023\n",
            "Epoch 507/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 33.5595 - val_loss: 35.2297\n",
            "Epoch 508/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 46.3179 - val_loss: 33.4731\n",
            "Epoch 509/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 40.2824 - val_loss: 32.3827\n",
            "Epoch 510/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 44.2470 - val_loss: 31.5360\n",
            "Epoch 511/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 52.9078 - val_loss: 30.8638\n",
            "Epoch 512/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 39.3904 - val_loss: 30.0935\n",
            "Epoch 513/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 36.0312 - val_loss: 29.1820\n",
            "Epoch 514/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 38.5895 - val_loss: 28.0778\n",
            "Epoch 515/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 41.8912 - val_loss: 27.1160\n",
            "Epoch 516/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 36.6349 - val_loss: 26.2751\n",
            "Epoch 517/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 47.5150 - val_loss: 26.5290\n",
            "Epoch 518/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 43.1832 - val_loss: 27.1556\n",
            "Epoch 519/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 45.8222 - val_loss: 27.4831\n",
            "Epoch 520/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 42.0089 - val_loss: 28.5164\n",
            "Epoch 521/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 50.3366 - val_loss: 29.7451\n",
            "Epoch 522/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 32.7528 - val_loss: 30.9145\n",
            "Epoch 523/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 50.0263 - val_loss: 31.3249\n",
            "Epoch 524/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 40.6761 - val_loss: 31.1642\n",
            "Epoch 525/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 35.2470 - val_loss: 30.3533\n",
            "Epoch 526/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 40.3913 - val_loss: 30.3815\n",
            "Epoch 527/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 39.7473 - val_loss: 31.0766\n",
            "Epoch 528/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 36.4534 - val_loss: 31.7028\n",
            "Epoch 529/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 35.6323 - val_loss: 31.9023\n",
            "Epoch 530/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 45.3962 - val_loss: 30.6258\n",
            "Epoch 531/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 35.8616 - val_loss: 29.6216\n",
            "Epoch 532/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 42.9018 - val_loss: 28.9162\n",
            "Epoch 533/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 43.5974 - val_loss: 28.2800\n",
            "Epoch 534/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 59.0172 - val_loss: 28.2925\n",
            "Epoch 535/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 44.0007 - val_loss: 29.4414\n",
            "Epoch 536/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 35.1350 - val_loss: 29.6792\n",
            "Epoch 537/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 35.2955 - val_loss: 29.0022\n",
            "Epoch 538/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 33.0435 - val_loss: 28.9161\n",
            "Epoch 539/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 58.5911 - val_loss: 29.4234\n",
            "Epoch 540/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 37.3812 - val_loss: 30.7536\n",
            "Epoch 541/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 34.8950 - val_loss: 31.7920\n",
            "Epoch 542/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 38.7011 - val_loss: 33.6982\n",
            "Epoch 543/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 37.9550 - val_loss: 35.2161\n",
            "Epoch 544/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 31.1584 - val_loss: 36.6265\n",
            "Epoch 545/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 36.6779 - val_loss: 37.8730\n",
            "Epoch 546/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 36.9248 - val_loss: 37.3991\n",
            "Epoch 547/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 32.7539 - val_loss: 36.8409\n",
            "Epoch 548/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 37.7657 - val_loss: 36.9011\n",
            "Epoch 549/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 41.5006 - val_loss: 36.2460\n",
            "Epoch 550/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 44.6516 - val_loss: 36.1490\n",
            "Epoch 551/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 45.5303 - val_loss: 35.7633\n",
            "Epoch 552/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 38.2043 - val_loss: 36.3536\n",
            "Epoch 553/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 39.5751 - val_loss: 37.6179\n",
            "Epoch 554/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 35.6734 - val_loss: 37.4813\n",
            "Epoch 555/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 44.0908 - val_loss: 36.9138\n",
            "Epoch 556/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 31.0145 - val_loss: 36.7292\n",
            "Epoch 557/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 40.3984 - val_loss: 36.2276\n",
            "Epoch 558/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 40.7602 - val_loss: 35.6354\n",
            "Epoch 559/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 48.0004 - val_loss: 33.8674\n",
            "Epoch 560/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 36.7728 - val_loss: 32.3944\n",
            "Epoch 561/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 32.6334 - val_loss: 30.8356\n",
            "Epoch 562/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 31.6829 - val_loss: 30.0873\n",
            "Epoch 563/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 34.1822 - val_loss: 29.4435\n",
            "Epoch 564/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 35.9814 - val_loss: 28.7810\n",
            "Epoch 565/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 38.5502 - val_loss: 28.9070\n",
            "Epoch 566/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 35.3297 - val_loss: 29.5128\n",
            "Epoch 567/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 41.0535 - val_loss: 30.2212\n",
            "Epoch 568/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 30.7073 - val_loss: 29.5055\n",
            "Epoch 569/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 32.2412 - val_loss: 28.5498\n",
            "Epoch 570/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 38.6382 - val_loss: 27.9813\n",
            "Epoch 571/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 39.8710 - val_loss: 27.2319\n",
            "Epoch 572/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 39.8230 - val_loss: 26.8977\n",
            "Epoch 573/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 38.2812 - val_loss: 25.5624\n",
            "Epoch 574/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 41.0891 - val_loss: 23.8471\n",
            "Epoch 575/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 33.6941 - val_loss: 23.6627\n",
            "Epoch 576/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 37.6979 - val_loss: 24.1821\n",
            "Epoch 577/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 33.9788 - val_loss: 25.1211\n",
            "Epoch 578/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 33.8712 - val_loss: 25.1343\n",
            "Epoch 579/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 40.9371 - val_loss: 25.5513\n",
            "Epoch 580/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 33.3735 - val_loss: 25.6734\n",
            "Epoch 581/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 40.8847 - val_loss: 26.4410\n",
            "Epoch 582/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.8013 - val_loss: 27.1052\n",
            "Epoch 583/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 40.6666 - val_loss: 27.2583\n",
            "Epoch 584/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 41.3010 - val_loss: 27.3028\n",
            "Epoch 585/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 38.5322 - val_loss: 27.9225\n",
            "Epoch 586/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 38.8077 - val_loss: 28.0762\n",
            "Epoch 587/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 43.6979 - val_loss: 27.6536\n",
            "Epoch 588/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 39.4914 - val_loss: 28.0177\n",
            "Epoch 589/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 37.8738 - val_loss: 28.6429\n",
            "Epoch 590/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 33.1076 - val_loss: 29.2568\n",
            "Epoch 591/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 45.9839 - val_loss: 29.5339\n",
            "Epoch 592/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 34.0380 - val_loss: 30.6081\n",
            "Epoch 593/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 32.6041 - val_loss: 31.9475\n",
            "Epoch 594/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - loss: 30.5991 - val_loss: 32.7546\n",
            "Epoch 595/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 35.8636 - val_loss: 33.4109\n",
            "Epoch 596/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 37.5404 - val_loss: 34.9221\n",
            "Epoch 597/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 38.9537 - val_loss: 34.3951\n",
            "Epoch 598/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 32.5601 - val_loss: 34.2869\n",
            "Epoch 599/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 42.6527 - val_loss: 34.0825\n",
            "Epoch 600/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 39.5239 - val_loss: 34.0739\n",
            "Epoch 601/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 31.5305 - val_loss: 33.1571\n",
            "Epoch 602/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 45.7787 - val_loss: 31.5319\n",
            "Epoch 603/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 36.9864 - val_loss: 30.0576\n",
            "Epoch 604/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 38.1016 - val_loss: 28.4992\n",
            "Epoch 605/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.6789 - val_loss: 26.6485\n",
            "Epoch 606/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 41.1682 - val_loss: 25.8924\n",
            "Epoch 607/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 32.0456 - val_loss: 24.9085\n",
            "Epoch 608/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 43.1702 - val_loss: 24.3952\n",
            "Epoch 609/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 41.9008 - val_loss: 24.3540\n",
            "Epoch 610/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 37.6991 - val_loss: 24.9789\n",
            "Epoch 611/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 36.6343 - val_loss: 26.6323\n",
            "Epoch 612/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 45.9348 - val_loss: 27.7692\n",
            "Epoch 613/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 31.0461 - val_loss: 28.7429\n",
            "Epoch 614/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - loss: 45.2120 - val_loss: 29.4219\n",
            "Epoch 615/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 39.5723 - val_loss: 29.9701\n",
            "Epoch 616/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 35.4752 - val_loss: 31.1046\n",
            "Epoch 617/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 45.3800 - val_loss: 30.7919\n",
            "Epoch 618/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 35.9582 - val_loss: 30.4629\n",
            "Epoch 619/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 41.2535 - val_loss: 30.3521\n",
            "Epoch 620/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 36.4180 - val_loss: 31.0028\n",
            "Epoch 621/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 41.2605 - val_loss: 31.3309\n",
            "Epoch 622/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 41.6138 - val_loss: 31.9848\n",
            "Epoch 623/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 42.2244 - val_loss: 32.1098\n",
            "Epoch 624/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 38.6504 - val_loss: 31.3692\n",
            "Epoch 625/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 38.3001 - val_loss: 30.8789\n",
            "Epoch 626/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 34.4932 - val_loss: 30.6196\n",
            "Epoch 627/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.8429 - val_loss: 30.0322\n",
            "Epoch 628/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 33.2083 - val_loss: 29.2002\n",
            "Epoch 629/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 46.3554 - val_loss: 27.9815\n",
            "Epoch 630/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 42.0167 - val_loss: 27.2227\n",
            "Epoch 631/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 34.2129 - val_loss: 26.6365\n",
            "Epoch 632/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 36.9923 - val_loss: 25.7524\n",
            "Epoch 633/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 34.2870 - val_loss: 26.0129\n",
            "Epoch 634/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 42.3408 - val_loss: 26.7509\n",
            "Epoch 635/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 42.7376 - val_loss: 25.9088\n",
            "Epoch 636/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 31.8706 - val_loss: 25.2911\n",
            "Epoch 637/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 30.4249 - val_loss: 25.4668\n",
            "Epoch 638/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 41.5033 - val_loss: 25.6563\n",
            "Epoch 639/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 34.9199 - val_loss: 26.2271\n",
            "Epoch 640/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 36.2469 - val_loss: 27.2418\n",
            "Epoch 641/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 39.2516 - val_loss: 27.7458\n",
            "Epoch 642/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 36.7052 - val_loss: 28.1031\n",
            "Epoch 643/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 37.7716 - val_loss: 28.3422\n",
            "Epoch 644/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 38.4572 - val_loss: 29.1291\n",
            "Epoch 645/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 37.1487 - val_loss: 29.1410\n",
            "Epoch 646/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 36.3951 - val_loss: 29.7722\n",
            "Epoch 647/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 39.0572 - val_loss: 31.0443\n",
            "Epoch 648/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 37.9994 - val_loss: 32.2492\n",
            "Epoch 649/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 37.2725 - val_loss: 32.6482\n",
            "Epoch 650/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 35.7601 - val_loss: 32.8328\n",
            "Epoch 651/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 49.9528 - val_loss: 31.3352\n",
            "Epoch 652/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 35.8959 - val_loss: 31.0244\n",
            "Epoch 653/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 45.7301 - val_loss: 31.5473\n",
            "Epoch 654/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 38.0583 - val_loss: 33.0357\n",
            "Epoch 655/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 34.7671 - val_loss: 34.2254\n",
            "Epoch 656/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 27.5884 - val_loss: 35.9485\n",
            "Epoch 657/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 39.6779 - val_loss: 36.3139\n",
            "Epoch 658/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 42.9329 - val_loss: 36.2345\n",
            "Epoch 659/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 44.9729 - val_loss: 36.3204\n",
            "Epoch 660/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 32.1914 - val_loss: 36.9381\n",
            "Epoch 661/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 33.0418 - val_loss: 36.4897\n",
            "Epoch 662/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 34.8706 - val_loss: 36.0996\n",
            "Epoch 663/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 39.9810 - val_loss: 36.6181\n",
            "Epoch 664/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 32.1336 - val_loss: 36.5816\n",
            "Epoch 665/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 40.4947 - val_loss: 35.8266\n",
            "Epoch 666/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 37.6413 - val_loss: 35.1488\n",
            "Epoch 667/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 34.8877 - val_loss: 35.1836\n",
            "Epoch 668/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 35.1637 - val_loss: 34.7646\n",
            "Epoch 669/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 41.3654 - val_loss: 33.1060\n",
            "Epoch 670/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 48.2922 - val_loss: 31.2068\n",
            "Epoch 671/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 33.3082 - val_loss: 29.7524\n",
            "Epoch 672/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 43.1160 - val_loss: 27.9870\n",
            "Epoch 673/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 40.3498 - val_loss: 26.7234\n",
            "Epoch 674/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 37.4343 - val_loss: 26.4171\n",
            "Epoch 675/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 42.2309 - val_loss: 25.4060\n",
            "Epoch 676/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 30.8265 - val_loss: 25.0899\n",
            "Epoch 677/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 48.7255 - val_loss: 24.9668\n",
            "Epoch 678/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 37.1975 - val_loss: 25.0146\n",
            "Epoch 679/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 45.4343 - val_loss: 25.9993\n",
            "Epoch 680/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 42.6425 - val_loss: 26.4244\n",
            "Epoch 681/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 37.8882 - val_loss: 27.2452\n",
            "Epoch 682/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 44.0735 - val_loss: 28.2016\n",
            "Epoch 683/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 43.1680 - val_loss: 30.0499\n",
            "Epoch 684/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 52.7227 - val_loss: 31.9601\n",
            "Epoch 685/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 37.6177 - val_loss: 32.6732\n",
            "Epoch 686/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 40.0282 - val_loss: 31.9844\n",
            "Epoch 687/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 44.8116 - val_loss: 31.5931\n",
            "Epoch 688/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 41.2048 - val_loss: 30.5518\n",
            "Epoch 689/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 41.7212 - val_loss: 29.2434\n",
            "Epoch 690/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 38.3689 - val_loss: 27.2133\n",
            "Epoch 691/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 31.2306 - val_loss: 25.6589\n",
            "Epoch 692/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 31.6475 - val_loss: 24.8978\n",
            "Epoch 693/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 37.3011 - val_loss: 24.6220\n",
            "Epoch 694/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 46.5713 - val_loss: 24.7757\n",
            "Epoch 695/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 35.1893 - val_loss: 25.1623\n",
            "Epoch 696/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 35.4912 - val_loss: 25.2331\n",
            "Epoch 697/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 41.8603 - val_loss: 25.2075\n",
            "Epoch 698/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 33.2495 - val_loss: 26.0031\n",
            "Epoch 699/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 46.6210 - val_loss: 26.5413\n",
            "Epoch 700/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 42.3222 - val_loss: 27.0770\n",
            "Epoch 701/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 37.4647 - val_loss: 27.6706\n",
            "Epoch 702/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 24.7201 - val_loss: 27.1421\n",
            "Epoch 703/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 37.1316 - val_loss: 26.6752\n",
            "Epoch 704/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 38.6504 - val_loss: 26.3938\n",
            "Epoch 705/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - loss: 34.5623 - val_loss: 24.7734\n",
            "Epoch 706/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 44.8884 - val_loss: 23.8633\n",
            "Epoch 707/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 26.8378 - val_loss: 23.2673\n",
            "Epoch 708/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 36.4731 - val_loss: 22.8031\n",
            "Epoch 709/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 33.2692 - val_loss: 22.3620\n",
            "Epoch 710/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 32.3179 - val_loss: 21.8555\n",
            "Epoch 711/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 33.5414 - val_loss: 21.4409\n",
            "Epoch 712/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 37.2480 - val_loss: 20.9169\n",
            "Epoch 713/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 32.6743 - val_loss: 20.5383\n",
            "Epoch 714/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 33.8421 - val_loss: 20.3114\n",
            "Epoch 715/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 29.6343 - val_loss: 19.9782\n",
            "Epoch 716/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 30.3741 - val_loss: 20.2673\n",
            "Epoch 717/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 31.7639 - val_loss: 19.8518\n",
            "Epoch 718/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 29.1667 - val_loss: 19.8315\n",
            "Epoch 719/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 38.5517 - val_loss: 20.3546\n",
            "Epoch 720/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 32.0596 - val_loss: 20.2008\n",
            "Epoch 721/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 34.6823 - val_loss: 20.3858\n",
            "Epoch 722/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 37.3432 - val_loss: 21.1077\n",
            "Epoch 723/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 31.2245 - val_loss: 21.6885\n",
            "Epoch 724/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 46.9594 - val_loss: 22.6588\n",
            "Epoch 725/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 32.4547 - val_loss: 23.8715\n",
            "Epoch 726/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 38.5708 - val_loss: 24.6469\n",
            "Epoch 727/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 28.0060 - val_loss: 25.3732\n",
            "Epoch 728/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 38.1687 - val_loss: 27.0837\n",
            "Epoch 729/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 28.8667 - val_loss: 29.0518\n",
            "Epoch 730/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 41.5886 - val_loss: 30.2576\n",
            "Epoch 731/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 46.9310 - val_loss: 29.9636\n",
            "Epoch 732/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 29.9447 - val_loss: 30.3986\n",
            "Epoch 733/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 34.4317 - val_loss: 30.4761\n",
            "Epoch 734/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 38.9838 - val_loss: 30.8197\n",
            "Epoch 735/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 29.7692 - val_loss: 31.0957\n",
            "Epoch 736/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 40.6721 - val_loss: 31.3305\n",
            "Epoch 737/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 44.9003 - val_loss: 30.6676\n",
            "Epoch 738/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 32.4611 - val_loss: 30.1633\n",
            "Epoch 739/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 33.0128 - val_loss: 29.8479\n",
            "Epoch 740/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 35.5248 - val_loss: 29.6112\n",
            "Epoch 741/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 37.2554 - val_loss: 29.9769\n",
            "Epoch 742/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 31.8704 - val_loss: 30.2974\n",
            "Epoch 743/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 37.5885 - val_loss: 29.9165\n",
            "Epoch 744/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 39.5312 - val_loss: 30.4863\n",
            "Epoch 745/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 32.3732 - val_loss: 30.3092\n",
            "Epoch 746/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 34.5156 - val_loss: 31.5628\n",
            "Epoch 747/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 36.1631 - val_loss: 32.9389\n",
            "Epoch 748/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 28.6106 - val_loss: 32.5531\n",
            "Epoch 749/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 44.9947 - val_loss: 33.5126\n",
            "Epoch 750/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 44.2105 - val_loss: 34.1814\n",
            "Epoch 751/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 30.5192 - val_loss: 35.0328\n",
            "Epoch 752/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 31.3201 - val_loss: 35.3186\n",
            "Epoch 753/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 34.7715 - val_loss: 36.0449\n",
            "Epoch 754/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 32.2767 - val_loss: 35.5070\n",
            "Epoch 755/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 40.3097 - val_loss: 34.9572\n",
            "Epoch 756/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 33.3509 - val_loss: 34.2457\n",
            "Epoch 757/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 29.4158 - val_loss: 33.3070\n",
            "Epoch 758/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 38.0910 - val_loss: 31.5686\n",
            "Epoch 759/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 36.0092 - val_loss: 30.8490\n",
            "Epoch 760/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 32.8258 - val_loss: 30.4359\n",
            "Epoch 761/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 33.2514 - val_loss: 29.6322\n",
            "Epoch 762/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 36.3917 - val_loss: 29.1951\n",
            "Epoch 763/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 46.6598 - val_loss: 27.8827\n",
            "Epoch 764/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 27.9812 - val_loss: 27.4610\n",
            "Epoch 765/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 40.6091 - val_loss: 28.3172\n",
            "Epoch 766/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 27.9418 - val_loss: 29.1060\n",
            "Epoch 767/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 36.1138 - val_loss: 29.6627\n",
            "Epoch 768/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 34.4971 - val_loss: 30.2650\n",
            "Epoch 769/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 34.4357 - val_loss: 30.7759\n",
            "Epoch 770/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 34.8309 - val_loss: 30.9721\n",
            "Epoch 771/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 34.9020 - val_loss: 30.3591\n",
            "Epoch 772/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 33.2807 - val_loss: 29.5467\n",
            "Epoch 773/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 43.8192 - val_loss: 29.7476\n",
            "Epoch 774/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.6922 - val_loss: 28.8338\n",
            "Epoch 775/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 34.7178 - val_loss: 28.1099\n",
            "Epoch 776/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 35.7944 - val_loss: 27.8632\n",
            "Epoch 777/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 35.3001 - val_loss: 28.1195\n",
            "Epoch 778/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 38.5735 - val_loss: 27.9931\n",
            "Epoch 779/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 32.6523 - val_loss: 28.2946\n",
            "Epoch 780/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 36.4191 - val_loss: 28.5704\n",
            "Epoch 781/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 31.5444 - val_loss: 28.6195\n",
            "Epoch 782/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 33.3182 - val_loss: 28.5264\n",
            "Epoch 783/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 27.4990 - val_loss: 28.5391\n",
            "Epoch 784/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 27.4948 - val_loss: 28.8465\n",
            "Epoch 785/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 34.1789 - val_loss: 29.1965\n",
            "Epoch 786/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 34.6567 - val_loss: 28.4791\n",
            "Epoch 787/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 29.8899 - val_loss: 27.5993\n",
            "Epoch 788/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 32.3295 - val_loss: 27.7163\n",
            "Epoch 789/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 38.5366 - val_loss: 26.7867\n",
            "Epoch 790/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 35.9665 - val_loss: 26.3667\n",
            "Epoch 791/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 36.8532 - val_loss: 25.8746\n",
            "Epoch 792/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 29.3755 - val_loss: 25.3246\n",
            "Epoch 793/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 29.7897 - val_loss: 24.8642\n",
            "Epoch 794/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 28.5229 - val_loss: 24.4111\n",
            "Epoch 795/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 30.0967 - val_loss: 24.7343\n",
            "Epoch 796/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 37.5751 - val_loss: 25.1254\n",
            "Epoch 797/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 29.4273 - val_loss: 25.8653\n",
            "Epoch 798/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 30.4187 - val_loss: 26.2884\n",
            "Epoch 799/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 32.9553 - val_loss: 26.7232\n",
            "Epoch 800/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 34.6255 - val_loss: 26.4226\n",
            "Epoch 801/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 30.8132 - val_loss: 26.4621\n",
            "Epoch 802/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 45.7198 - val_loss: 26.4179\n",
            "Epoch 803/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 33.9996 - val_loss: 26.1262\n",
            "Epoch 804/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 33.3364 - val_loss: 25.8797\n",
            "Epoch 805/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 29.5736 - val_loss: 25.8272\n",
            "Epoch 806/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 32.0454 - val_loss: 24.4347\n",
            "Epoch 807/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 36.0709 - val_loss: 23.9767\n",
            "Epoch 808/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 36.4783 - val_loss: 23.7970\n",
            "Epoch 809/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 38.0078 - val_loss: 23.4697\n",
            "Epoch 810/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 37.8201 - val_loss: 22.6909\n",
            "Epoch 811/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 34.7271 - val_loss: 22.1397\n",
            "Epoch 812/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 33.0569 - val_loss: 21.0897\n",
            "Epoch 813/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 35.2491 - val_loss: 20.6481\n",
            "Epoch 814/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 34.0212 - val_loss: 20.7707\n",
            "Epoch 815/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 43.3891 - val_loss: 21.2656\n",
            "Epoch 816/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 35.6229 - val_loss: 22.2454\n",
            "Epoch 817/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 31.2526 - val_loss: 22.8691\n",
            "Epoch 818/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 29.2252 - val_loss: 23.1719\n",
            "Epoch 819/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 29.0767 - val_loss: 23.4801\n",
            "Epoch 820/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 33.2214 - val_loss: 23.9907\n",
            "Epoch 821/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 29.8639 - val_loss: 23.8472\n",
            "Epoch 822/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 36.1270 - val_loss: 24.3425\n",
            "Epoch 823/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 34.4741 - val_loss: 24.9568\n",
            "Epoch 824/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 30.5572 - val_loss: 25.8840\n",
            "Epoch 825/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 43.4777 - val_loss: 27.2969\n",
            "Epoch 826/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 33.0772 - val_loss: 28.8404\n",
            "Epoch 827/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 28.0974 - val_loss: 30.2332\n",
            "Epoch 828/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 30.6510 - val_loss: 31.7789\n",
            "Epoch 829/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 40.6981 - val_loss: 32.2567\n",
            "Epoch 830/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 37.7997 - val_loss: 32.3859\n",
            "Epoch 831/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 34.7466 - val_loss: 32.4615\n",
            "Epoch 832/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 27.6852 - val_loss: 32.5246\n",
            "Epoch 833/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 31.2484 - val_loss: 31.8776\n",
            "Epoch 834/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 24.2697 - val_loss: 31.7258\n",
            "Epoch 835/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 29.7783 - val_loss: 30.9964\n",
            "Epoch 836/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 34.6134 - val_loss: 30.0707\n",
            "Epoch 837/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 30.0550 - val_loss: 29.7189\n",
            "Epoch 838/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 31.6138 - val_loss: 29.3181\n",
            "Epoch 839/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 30.8274 - val_loss: 29.0380\n",
            "Epoch 840/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 36.2101 - val_loss: 28.7843\n",
            "Epoch 841/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 33.1872 - val_loss: 28.2825\n",
            "Epoch 842/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 30.4048 - val_loss: 28.4828\n",
            "Epoch 843/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 36.9290 - val_loss: 28.4969\n",
            "Epoch 844/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 30.3891 - val_loss: 28.8828\n",
            "Epoch 845/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 28.5039 - val_loss: 28.9340\n",
            "Epoch 846/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 33.2683 - val_loss: 28.6236\n",
            "Epoch 847/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 44.1970 - val_loss: 28.1695\n",
            "Epoch 848/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 35.4406 - val_loss: 28.2026\n",
            "Epoch 849/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 43.1051 - val_loss: 27.4416\n",
            "Epoch 850/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 32.9862 - val_loss: 26.7549\n",
            "Epoch 851/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 31.9712 - val_loss: 26.2322\n",
            "Epoch 852/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 29.4358 - val_loss: 26.1726\n",
            "Epoch 853/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 31.6573 - val_loss: 26.1188\n",
            "Epoch 854/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 24.5362 - val_loss: 26.9112\n",
            "Epoch 855/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 34.2085 - val_loss: 28.1339\n",
            "Epoch 856/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 36.1344 - val_loss: 28.7137\n",
            "Epoch 857/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 28.3884 - val_loss: 28.3927\n",
            "Epoch 858/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 34.2360 - val_loss: 27.5825\n",
            "Epoch 859/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 23.1548 - val_loss: 27.1096\n",
            "Epoch 860/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 30.8155 - val_loss: 26.5288\n",
            "Epoch 861/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 23.1946 - val_loss: 25.8987\n",
            "Epoch 862/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 30.7764 - val_loss: 25.9494\n",
            "Epoch 863/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 39.7136 - val_loss: 26.0044\n",
            "Epoch 864/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 32.5678 - val_loss: 26.3340\n",
            "Epoch 865/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 29.9339 - val_loss: 26.7468\n",
            "Epoch 866/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 35.8337 - val_loss: 26.9791\n",
            "Epoch 867/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 30.1924 - val_loss: 26.6983\n",
            "Epoch 868/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 33.5567 - val_loss: 25.8641\n",
            "Epoch 869/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 38.0419 - val_loss: 25.1985\n",
            "Epoch 870/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 31.3484 - val_loss: 25.2188\n",
            "Epoch 871/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step - loss: 25.9726 - val_loss: 25.2173\n",
            "Epoch 872/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 37.6628 - val_loss: 24.7013\n",
            "Epoch 873/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 36.9813 - val_loss: 24.6075\n",
            "Epoch 874/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 35.5130 - val_loss: 24.2739\n",
            "Epoch 875/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 40.0999 - val_loss: 24.1995\n",
            "Epoch 876/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 35.7848 - val_loss: 24.5497\n",
            "Epoch 877/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - loss: 29.9876 - val_loss: 24.4318\n",
            "Epoch 878/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 26.9568 - val_loss: 24.4161\n",
            "Epoch 879/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 29.1495 - val_loss: 24.4111\n",
            "Epoch 880/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 26.7168 - val_loss: 24.6229\n",
            "Epoch 881/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 34.8110 - val_loss: 24.5756\n",
            "Epoch 882/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 37.9883 - val_loss: 24.7874\n",
            "Epoch 883/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 31.5330 - val_loss: 25.0683\n",
            "Epoch 884/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 36.1590 - val_loss: 25.8776\n",
            "Epoch 885/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 25.4676 - val_loss: 25.6923\n",
            "Epoch 886/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 34.9352 - val_loss: 25.0076\n",
            "Epoch 887/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 30.0481 - val_loss: 25.0988\n",
            "Epoch 888/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 30.3078 - val_loss: 25.5550\n",
            "Epoch 889/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 30.3811 - val_loss: 25.4185\n",
            "Epoch 890/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 27.7729 - val_loss: 24.5818\n",
            "Epoch 891/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 26.6936 - val_loss: 23.8621\n",
            "Epoch 892/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 33.0494 - val_loss: 22.4682\n",
            "Epoch 893/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 30.3580 - val_loss: 21.3483\n",
            "Epoch 894/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 25.2344 - val_loss: 20.2605\n",
            "Epoch 895/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 44.4347 - val_loss: 19.6655\n",
            "Epoch 896/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 37.9997 - val_loss: 18.7481\n",
            "Epoch 897/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 25.9721 - val_loss: 19.0710\n",
            "Epoch 898/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 31.1429 - val_loss: 19.9051\n",
            "Epoch 899/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 31.6980 - val_loss: 20.3952\n",
            "Epoch 900/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 29.9749 - val_loss: 20.7803\n",
            "Epoch 901/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 30.5619 - val_loss: 21.2453\n",
            "Epoch 902/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 52.2016 - val_loss: 22.3776\n",
            "Epoch 903/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 26.0237 - val_loss: 23.2502\n",
            "Epoch 904/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 25.0122 - val_loss: 23.9270\n",
            "Epoch 905/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 25.7276 - val_loss: 24.1050\n",
            "Epoch 906/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 29.1067 - val_loss: 24.3232\n",
            "Epoch 907/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 34.7511 - val_loss: 24.4619\n",
            "Epoch 908/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 33.1782 - val_loss: 24.9095\n",
            "Epoch 909/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 36.0072 - val_loss: 25.4915\n",
            "Epoch 910/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 28.5329 - val_loss: 25.3405\n",
            "Epoch 911/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 31.6491 - val_loss: 24.2631\n",
            "Epoch 912/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 39.8581 - val_loss: 23.6961\n",
            "Epoch 913/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 39.7928 - val_loss: 23.1502\n",
            "Epoch 914/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 24.2946 - val_loss: 23.5803\n",
            "Epoch 915/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 31.1064 - val_loss: 23.6052\n",
            "Epoch 916/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 22.6960 - val_loss: 23.7248\n",
            "Epoch 917/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 27.2031 - val_loss: 23.7564\n",
            "Epoch 918/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 31.6736 - val_loss: 23.7591\n",
            "Epoch 919/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 31.8021 - val_loss: 23.1194\n",
            "Epoch 920/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 27.3827 - val_loss: 23.5828\n",
            "Epoch 921/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 41.6500 - val_loss: 23.8925\n",
            "Epoch 922/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 38.3129 - val_loss: 24.1322\n",
            "Epoch 923/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 33.5330 - val_loss: 23.8871\n",
            "Epoch 924/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 25.9260 - val_loss: 24.0455\n",
            "Epoch 925/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 31.7312 - val_loss: 23.6940\n",
            "Epoch 926/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 33.0171 - val_loss: 23.5547\n",
            "Epoch 927/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 32.0133 - val_loss: 23.6316\n",
            "Epoch 928/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 30.0329 - val_loss: 23.9541\n",
            "Epoch 929/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 36.1752 - val_loss: 24.7352\n",
            "Epoch 930/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 27.7835 - val_loss: 25.2916\n",
            "Epoch 931/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 24.3141 - val_loss: 25.3959\n",
            "Epoch 932/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 26.7862 - val_loss: 25.8517\n",
            "Epoch 933/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 26.9755 - val_loss: 25.6818\n",
            "Epoch 934/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 33.7309 - val_loss: 25.0165\n",
            "Epoch 935/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 28.8947 - val_loss: 24.2725\n",
            "Epoch 936/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 35.5567 - val_loss: 23.2311\n",
            "Epoch 937/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 32.8745 - val_loss: 22.4471\n",
            "Epoch 938/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 32.1533 - val_loss: 22.1022\n",
            "Epoch 939/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 32.3480 - val_loss: 21.7042\n",
            "Epoch 940/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 56.7427 - val_loss: 21.2068\n",
            "Epoch 941/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 30.7737 - val_loss: 21.0484\n",
            "Epoch 942/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 31.3775 - val_loss: 21.3381\n",
            "Epoch 943/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 31.1277 - val_loss: 21.4249\n",
            "Epoch 944/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 38.2218 - val_loss: 21.9369\n",
            "Epoch 945/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 30.9375 - val_loss: 22.4846\n",
            "Epoch 946/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 31.1111 - val_loss: 23.3165\n",
            "Epoch 947/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 39.1035 - val_loss: 23.4936\n",
            "Epoch 948/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 27.7154 - val_loss: 23.0747\n",
            "Epoch 949/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 28.5794 - val_loss: 23.2674\n",
            "Epoch 950/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 29.7953 - val_loss: 22.9297\n",
            "Epoch 951/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 28.1061 - val_loss: 22.4117\n",
            "Epoch 952/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 29.7746 - val_loss: 21.4817\n",
            "Epoch 953/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 31.7799 - val_loss: 20.2426\n",
            "Epoch 954/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 40.7572 - val_loss: 19.1725\n",
            "Epoch 955/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 33.7753 - val_loss: 18.6098\n",
            "Epoch 956/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 30.0986 - val_loss: 18.6599\n",
            "Epoch 957/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 35.5566 - val_loss: 19.2759\n",
            "Epoch 958/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 24.2189 - val_loss: 19.4929\n",
            "Epoch 959/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 31.2524 - val_loss: 20.0932\n",
            "Epoch 960/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 28.5398 - val_loss: 20.6896\n",
            "Epoch 961/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 32.3253 - val_loss: 21.0169\n",
            "Epoch 962/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 27.3007 - val_loss: 21.1229\n",
            "Epoch 963/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 37.7411 - val_loss: 20.8068\n",
            "Epoch 964/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 40.0745 - val_loss: 20.6680\n",
            "Epoch 965/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 30.2918 - val_loss: 20.9816\n",
            "Epoch 966/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 34.9141 - val_loss: 21.0713\n",
            "Epoch 967/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 32.2929 - val_loss: 20.9012\n",
            "Epoch 968/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 33.7392 - val_loss: 20.8125\n",
            "Epoch 969/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 37.0299 - val_loss: 21.3048\n",
            "Epoch 970/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 30.3115 - val_loss: 20.9870\n",
            "Epoch 971/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 28.5780 - val_loss: 20.6127\n",
            "Epoch 972/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 30.5514 - val_loss: 20.0486\n",
            "Epoch 973/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 26.1723 - val_loss: 19.9074\n",
            "Epoch 974/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 33.7374 - val_loss: 19.9334\n",
            "Epoch 975/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 28.4659 - val_loss: 20.3685\n",
            "Epoch 976/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 34.2502 - val_loss: 20.3384\n",
            "Epoch 977/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 27.9281 - val_loss: 20.5034\n",
            "Epoch 978/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 34.8412 - val_loss: 20.6877\n",
            "Epoch 979/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 37.9739 - val_loss: 20.5359\n",
            "Epoch 980/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 25.4849 - val_loss: 19.9803\n",
            "Epoch 981/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 35.8357 - val_loss: 20.1320\n",
            "Epoch 982/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 25.9420 - val_loss: 20.4011\n",
            "Epoch 983/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 35.0758 - val_loss: 20.0983\n",
            "Epoch 984/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 26.7028 - val_loss: 20.1254\n",
            "Epoch 985/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 28.7812 - val_loss: 19.8463\n",
            "Epoch 986/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 29.8966 - val_loss: 20.0130\n",
            "Epoch 987/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 31.0187 - val_loss: 20.2967\n",
            "Epoch 988/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 29.7784 - val_loss: 20.3753\n",
            "Epoch 989/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 36.0209 - val_loss: 20.6465\n",
            "Epoch 990/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 27.0109 - val_loss: 21.2391\n",
            "Epoch 991/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 34.4730 - val_loss: 21.5483\n",
            "Epoch 992/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 27.7073 - val_loss: 21.4650\n",
            "Epoch 993/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 30.3727 - val_loss: 22.0058\n",
            "Epoch 994/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 29.1598 - val_loss: 22.6903\n",
            "Epoch 995/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 33.9673 - val_loss: 23.5003\n",
            "Epoch 996/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 24.1925 - val_loss: 24.1929\n",
            "Epoch 997/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 27.8764 - val_loss: 24.8348\n",
            "Epoch 998/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - loss: 22.5147 - val_loss: 25.3664\n",
            "Epoch 999/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 32.2020 - val_loss: 25.5588\n",
            "Epoch 1000/1000\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 30.0378 - val_loss: 25.5896\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Train Metrics:\n",
            "MSE: 7.268909173853611\n",
            "R^2: 0.9816640313494466\n",
            "RMSE: 2.6960914624421797\n",
            "MAE: 1.723707439045847\n",
            "MAPE: 19.039103321970778\n",
            "VAF: 0.9816766765750261\n",
            "RSR: 0.1354103712813515\n",
            "WMAPE: 4.014430177455915\n",
            "A-20: 93.00411522633745\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 27.046020633981364\n",
            "R^2: 0.9500397719332415\n",
            "RMSE: 5.20057887489281\n",
            "MAE: 3.7801090952138434\n",
            "MAPE: 30.289954318228602\n",
            "VAF: 0.9504119779743624\n",
            "RSR: 0.22351784731148094\n",
            "WMAPE: 8.017276555307612\n",
            "A-20: 86.88524590163934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vgZNUSXP-Zk"
      },
      "source": [
        "**hybrid**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from numpy.random import uniform\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert data to tensors for PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "\n",
        "# Neural Network Model\n",
        "class EnhancedNeuralNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(EnhancedNeuralNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize and train Enhanced Neural Network\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_model = EnhancedNeuralNet(input_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "num_epochs = 2000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    nn_model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = nn_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "nn_model.eval()\n",
        "y_pred_nn = nn_model(X_test_tensor).detach().numpy().flatten()\n",
        "\n",
        "# Train other models\n",
        "gpr = GaussianProcessRegressor(kernel=C(1.0) * RBF() + WhiteKernel(), n_restarts_optimizer=15)\n",
        "gpr.fit(X_train_scaled, y_train)\n",
        "y_pred_gpr, _ = gpr.predict(X_test_scaled, return_std=True)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=30)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf.predict(X_test_scaled)\n",
        "\n",
        "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)\n",
        "gb.fit(X_train_scaled, y_train)\n",
        "y_pred_gb = gb.predict(X_test_scaled)\n",
        "\n",
        "svr = SVR(C=6.0, epsilon=0.1)\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "y_pred_svr = svr.predict(X_test_scaled)\n",
        "\n",
        "# Optimize ensemble weights using Grey Wolf Optimizer\n",
        "class GreyWolfOptimizer:\n",
        "    def __init__(self, func, lb, ub, dim, population_size=20, max_iter=500):\n",
        "        self.func = func\n",
        "        self.lb = np.array(lb)\n",
        "        self.ub = np.array(ub)\n",
        "        self.dim = dim\n",
        "        self.population_size = population_size\n",
        "        self.max_iter = max_iter\n",
        "        self.population = uniform(self.lb, self.ub, (self.population_size, self.dim))\n",
        "        self.alpha, self.beta, self.delta = None, None, None\n",
        "\n",
        "    def optimize(self):\n",
        "        for t in range(self.max_iter):\n",
        "            fitness = np.array([self.func(ind) for ind in self.population])\n",
        "            sorted_indices = np.argsort(fitness)\n",
        "            self.alpha = self.population[sorted_indices[0]]\n",
        "            self.beta = self.population[sorted_indices[1]]\n",
        "            self.delta = self.population[sorted_indices[2]]\n",
        "\n",
        "            for i in range(self.population_size):\n",
        "                A1 = 2 * np.random.rand(self.dim) - 1\n",
        "                C1 = 2 * np.random.rand(self.dim)\n",
        "                D_alpha = abs(C1 * self.alpha - self.population[i])\n",
        "                X1 = self.alpha - A1 * D_alpha\n",
        "\n",
        "                A2 = 2 * np.random.rand(self.dim) - 1\n",
        "                C2 = 2 * np.random.rand(self.dim)\n",
        "                D_beta = abs(C2 * self.beta - self.population[i])\n",
        "                X2 = self.beta - A2 * D_beta\n",
        "\n",
        "                A3 = 2 * np.random.rand(self.dim) - 1\n",
        "                C3 = 2 * np.random.rand(self.dim)\n",
        "                D_delta = abs(C3 * self.delta - self.population[i])\n",
        "                X3 = self.delta - A3 * D_delta\n",
        "\n",
        "                self.population[i] = np.clip((X1 + X2 + X3) / 3.0, self.lb, self.ub)\n",
        "\n",
        "        return self.alpha\n",
        "\n",
        "# Define loss function for GWO\n",
        "bounds = [(0, 1)] * 5\n",
        "\n",
        "def ensemble_loss(weights):\n",
        "    weights = weights / np.sum(weights)  # Normalize weights\n",
        "    y_pred_ensemble = (\n",
        "        weights[0] * y_pred_nn +\n",
        "        weights[1] * y_pred_gpr +\n",
        "        weights[2] * y_pred_rf +\n",
        "        weights[3] * y_pred_gb +\n",
        "        weights[4] * y_pred_svr\n",
        "    )\n",
        "    return mean_squared_error(y_test, y_pred_ensemble)\n",
        "\n",
        "# Run GWO\n",
        "gwo = GreyWolfOptimizer(func=ensemble_loss, lb=[0]*5, ub=[1]*5, dim=5, population_size=100, max_iter=1000)\n",
        "best_weights = gwo.optimize()\n",
        "best_weights = best_weights / np.sum(best_weights)  # Normalize weights\n",
        "\n",
        "# Final Ensemble Predictions\n",
        "y_pred_ensemble = (\n",
        "    best_weights[0] * y_pred_nn +\n",
        "    best_weights[1] * y_pred_gpr +\n",
        "    best_weights[2] * y_pred_rf +\n",
        "    best_weights[3] * y_pred_gb +\n",
        "    best_weights[4] * y_pred_svr\n",
        ")\n",
        "\n",
        "# Calculate ensemble predictions for training data\n",
        "y_pred_ensemble_train = (\n",
        "    best_weights[0] * nn_model(X_train_tensor).detach().numpy().flatten() +\n",
        "    best_weights[1] * gpr.predict(X_train_scaled) +\n",
        "    best_weights[2] * rf.predict(X_train_scaled) +\n",
        "    best_weights[3] * gb.predict(X_train_scaled) +\n",
        "    best_weights[4] * svr.predict(X_train_scaled)\n",
        ")\n",
        "\n",
        "# Calculate metrics for training and test data\n",
        "train_metrics = calculate_metrics(y_train, y_pred_ensemble_train)\n",
        "test_metrics = calculate_metrics(y_test, y_pred_ensemble)\n",
        "\n",
        "# Unpack metrics\n",
        "train_mse, train_r2, train_rmse, train_mae, train_mape, train_vaf, train_rsr, train_wmape, train_a20 = train_metrics\n",
        "test_mse, test_r2, test_rmse, test_mae, test_mape, test_vaf, test_rsr, test_wmape, test_a20 = test_metrics\n",
        "\n",
        "# Print results\n",
        "print(\"Advanced Hybrid Model with GWO Optimization:\")\n",
        "print(f\"Best Weights: {best_weights}\")\n",
        "print(\"\\nTrain Metrics:\")\n",
        "print(f\"MSE: {train_mse:.4f}\")\n",
        "print(f\"R^2: {train_r2:.4f}\")\n",
        "print(f\"RMSE: {train_rmse:.4f}\")\n",
        "print(f\"MAE: {train_mae:.4f}\")\n",
        "print(f\"MAPE: {train_mape:.4f}%\")\n",
        "print(f\"VAF: {train_vaf:.4f}\")\n",
        "print(f\"RSR: {train_rsr:.4f}\")\n",
        "print(f\"WMAPE: {train_wmape:.4f}%\")\n",
        "print(f\"A-20: {train_a20:.4f}%\")\n",
        "print(\"\\nTest Metrics:\")\n",
        "print(f\"MSE: {test_mse:.4f}\")\n",
        "print(f\"R^2: {test_r2:.4f}\")\n",
        "print(f\"RMSE: {test_rmse:.4f}\")\n",
        "print(f\"MAE: {test_mae:.4f}\")\n",
        "print(f\"MAPE: {test_mape:.4f}%\")\n",
        "print(f\"VAF: {test_vaf:.4f}\")\n",
        "print(f\"RSR: {test_rsr:.4f}\")\n",
        "print(f\"WMAPE: {test_wmape:.4f}%\")\n",
        "print(f\"A-20: {test_a20:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Bof7jivFqY",
        "outputId": "c70951de-f89c-4b0e-be1a-3327554c8aeb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced Hybrid Model with GWO Optimization:\n",
            "Best Weights: [6.81795647e-01 2.75779243e-01 2.72404201e-14 4.24251098e-02\n",
            " 2.16108750e-16]\n",
            "\n",
            "Train Metrics:\n",
            "MSE: 1.3474\n",
            "R^2: 0.9966\n",
            "RMSE: 1.1608\n",
            "MAE: 0.8551\n",
            "MAPE: 12.9383%\n",
            "VAF: 0.9966\n",
            "RSR: 0.0583\n",
            "WMAPE: 1.9916%\n",
            "A-20: 95.8848%\n",
            "\n",
            "Test Metrics:\n",
            "MSE: 25.0028\n",
            "R^2: 0.9538\n",
            "RMSE: 5.0003\n",
            "MAE: 2.9884\n",
            "MAPE: 18.3471%\n",
            "VAF: 0.9580\n",
            "RSR: 0.2149\n",
            "WMAPE: 6.3382%\n",
            "A-20: 88.5246%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Plot**"
      ],
      "metadata": {
        "id": "Me9OosfJrJsJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqlY-nS8lnv3"
      },
      "source": [
        " Heatmap plot & Scatter matrix plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnGMRhQm7WEq",
        "outputId": "ea53f574-c2ad-4741-86c4-a636d055e559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All plots have been saved in the directory: /content/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import pi\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Function to move the \"Strength\" column to the end\n",
        "def move_strength_column_to_end(df):\n",
        "    if 'Strength' in df.columns:\n",
        "        # Remove the \"Strength\" column and append it to the end\n",
        "        strength_column = df.pop('Strength')\n",
        "        df['Strength'] = strength_column\n",
        "    return df\n",
        "\n",
        "# 1. Heatmap plot\n",
        "def plot_heatmap(df, save_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    correlation_matrix = df.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f',\n",
        "                cbar_kws={'label': 'Correlation Coefficient'}, annot_kws={'size': 10}, linewidths=0.5, linecolor='black')\n",
        "    plt.title('Heatmap Correlation')\n",
        "    plt.savefig(save_path + '/heatmap_correlation.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# 2. Scatter Matrix Plot\n",
        "def plot_scatter_matrix(df, save_path):\n",
        "    scatter_matrix(df, figsize=(12, 10), alpha=0.8, diagonal='hist')\n",
        "    plt.suptitle('Scatter Matrix Correlation', y=1.02)\n",
        "    plt.savefig(save_path + '/scatter_matrix_correlation.png', dpi=300, bbox_inches='tight')  # Save high-quality image\n",
        "    plt.close()\n",
        "\n",
        "# File path for CSV and save path for images\n",
        "file_path = 'Strength_ِDataset'\n",
        "save_path = ''\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = load_data(file_path)\n",
        "\n",
        "# Move the \"Strength\" column to the end\n",
        "df = move_strength_column_to_end(df)\n",
        "\n",
        "# Generate and save plots\n",
        "plot_heatmap(df, save_path)          # Heatmap plot\n",
        "plot_scatter_matrix(df, save_path)   # Scatter matrix plot\n",
        "\n",
        "print(f\"All plots have been saved in the directory: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYqU47Qpu3_M"
      },
      "source": [
        "Plot histogram with cumulative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMC5Gu4kBaCu",
        "outputId": "b9fbb7c4-4b15-41f0-9e6f-d63faca245bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created at: /content/graphs3\n",
            "Columns being analyzed: ['Cement', 'Pumice', 'SilicaFume', 'Powder', 'W/P', 'SP', 'CA/FA', 'Type', 'Strength']\n",
            "\n",
            "Analyzing column: Cement\n",
            "Plot saved as: /content/graphs3/Cement_histogram.png\n",
            "Analyzing column: Pumice\n",
            "Plot saved as: /content/graphs3/Pumice_histogram.png\n",
            "Analyzing column: SilicaFume\n",
            "Plot saved as: /content/graphs3/SilicaFume_histogram.png\n",
            "Analyzing column: Powder\n",
            "Plot saved as: /content/graphs3/Powder_histogram.png\n",
            "Analyzing column: W/P\n",
            "Plot saved as: /content/graphs3/W_P_histogram.png\n",
            "Analyzing column: SP\n",
            "Plot saved as: /content/graphs3/SP_histogram.png\n",
            "Analyzing column: CA/FA\n",
            "Plot saved as: /content/graphs3/CA_FA_histogram.png\n",
            "Analyzing column: Type\n",
            "Plot saved as: /content/graphs3/Type_histogram.png\n",
            "Analyzing column: Strength\n",
            "Plot saved as: /content/graphs3/Strength_histogram.png\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to plot histogram with cumulative percentage\n",
        "def plot_histogram_with_cumulative(df, column, save_path=None):\n",
        "    # Create histogram\n",
        "    counts, bins = np.histogram(df[column].dropna(), bins=10)  # Set number of bins and drop NaN values\n",
        "\n",
        "    # Calculate cumulative percentage\n",
        "    cumulative_percentage = np.cumsum(counts) / np.sum(counts) * 100\n",
        "\n",
        "    # Plot the figure\n",
        "    fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Primary axis (histogram)\n",
        "    ax1.bar(bins[:-1], counts, width=np.diff(bins), edgecolor='black', color='orange', alpha=0.7, label='Frequency')\n",
        "    ax1.set_xlabel(column)\n",
        "    ax1.set_ylabel('Frequency', color='orange')\n",
        "    ax1.tick_params(axis='y', labelcolor='orange')\n",
        "\n",
        "    # Secondary axis (cumulative percentage)\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(bins[:-1], cumulative_percentage, color='teal', linestyle='--', marker='o', label='Cumulative %')\n",
        "    ax2.set_ylabel('Cumulative %', color='teal')\n",
        "    ax2.tick_params(axis='y', labelcolor='teal')\n",
        "\n",
        "    # Title and legend\n",
        "    plt.title(f'Histogram and Cumulative Percentage for {column}')\n",
        "    fig.legend(loc='upper left', bbox_to_anchor=(0.15, 0.85))\n",
        "\n",
        "    # Save the plot\n",
        "    if save_path:\n",
        "        # Replace invalid characters in column name for file name\n",
        "        safe_column_name = column.replace('/', '_')  # Replace '/' with '_'\n",
        "        file_name = os.path.join(save_path, f\"{safe_column_name}_histogram.png\")\n",
        "        plt.savefig(file_name, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Plot saved as: {file_name}\")\n",
        "\n",
        "    plt.close(fig)\n",
        "\n",
        "# Main function to analyze specified columns\n",
        "def analyze_columns(file_path, column1, column2, columns_to_drop, save_path):\n",
        "    # Check and create directory for saving plots\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "        print(f\"Directory created at: {save_path}\")\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Exclude specified columns\n",
        "    columns_to_exclude = [column1, column2] + columns_to_drop\n",
        "    columns_to_analyze = [col for col in df.columns if col not in columns_to_exclude]\n",
        "\n",
        "    # Print the list of columns being analyzed\n",
        "    print(f\"Columns being analyzed: {columns_to_analyze}\\n\")\n",
        "\n",
        "    # Plot for each column\n",
        "    for column in columns_to_analyze:\n",
        "        print(f\"Analyzing column: {column}\")\n",
        "        plot_histogram_with_cumulative(df, column, save_path)\n",
        "\n",
        "# File path for CSV and save path for plots\n",
        "file_path = 'Strength_ِDataset'\n",
        "save_path = ''\n",
        "\n",
        "# Columns to define for exclusion\n",
        "column1 = 'coarse_grain'\n",
        "column2 = 'fine_grain'\n",
        "columns_to_drop = ['water', 'W_to_C', 'type', 'Pumice/powder', 'silica fume/powder', 'fine_agg/agg', 'Filler']\n",
        "\n",
        "# Execute the main function\n",
        "analyze_columns(file_path, column1, column2, columns_to_drop, save_path)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (Pyspark)",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}